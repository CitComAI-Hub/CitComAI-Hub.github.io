{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CitCom.ai | VRAIN","text":"Minimal Package for TEF nodes <p>Work package 3 (WP3) provides the Minimal Package for CitCom.ai TEF (Testing and Experimentation Facility) nodes, which contains comprehensive guides, toolboxes, and deployment frameworks to ensure interoperability across the wide range of TEF nodes by defining a common reference architecture. Based on this architecture, it also provides support and examples to develop AI services.</p> <p></p>"},{"location":"#package-components","title":"Package components:","text":"<ul> <li> <p> Getting Started</p> <p>Start by learning the basics of CitCom.ai and its common reference architecture.</p> <p> Learn more</p> </li> <li> <p> TEF Nodes</p> <p>Testing and Experimentation Facility (TEF) nodes.</p> <p> Learn more</p> </li> <li> <p> Toolbox</p> <p>A set of useful tools compatible with MIMs.</p> <p> Learn more</p> </li> <li> <p> AI Services</p> <p>Minimal interoperable AI services for platforms compatible with MIMs.</p> <p> Learn more</p> </li> <li> <p> Documentation</p> <p>Reports and deployment guides of different components.</p> <p> Learn more</p> </li> <li> <p> Data Catalog</p> <p>Centralized hub to keep track of available datasets.</p> <p> Learn more</p> </li> </ul>"},{"location":"welcome/","title":"Welcome","text":"<p>This site, curated by the Valencian Research Institute for Artificial Intelligence (VRAIN), serves as a comprehensive documentation hub for the CitCom.ai project, focusing on offering the Minimal Package for Testing and Experimentation Facilities (TEF) nodes. The Minimal Package contains comprehensive guides, toolboxes, and deployment frameworks to ensure interoperability across the CitCom.ai wide range of TEF nodes by defining a common reference architecture. Based on this architecture, it also provides support and examples for developing AI services.</p>"},{"location":"welcome/#about-citcomai","title":"About CitCom.ai","text":"<p>The CitCom.ai project is an ambitious European Union initiative aimed at establishing a comprehensive Testing and Experimentation Facilities (TEFs) for AI and robotics, tailored explicitly to smart and sustainable cities and communities. This project is designed to accelerate the adoption of AI technologies across Europe by providing real-world testing environments where companies can develop, test, and validate their AI-driven products and services. CitCom ensures that these technologies meet the latest EU regulations, including those related to data privacy and cybersecurity while supporting the creation of interoperable and ethically sound AI solutions. The project spans multiple countries, with over 30 participating organizations, and it emphasizes a coordinated approach to fostering innovation that aligns with the EU\u2019s goals for a greener and more digital future.</p> <p></p> <p></p> <p>Next steps</p> <p>Click on Getting started in the bottom navigation bar to advance to the next section.</p> <p>You can use the buttons in the bottom navigation bar to navigate between the previous and next pages or jump to a section with the side navigation bars.</p>"},{"location":"data_catalog/","title":"Data Catalog","text":"<p>The data catalog is a centralized hub to keep track of available datasets. It is regularly updated to include new data as it becomes available in any TEF node. If you want access to any dataset, please click \"Contact\" to reach the owners.</p> <p>How to add new datasets?</p> <p> Add New Datasets </p> Dataset Super Node TEF Node Site Data Model Sampling Time Historical Owner Get Access Waste Container South Spain Valencia gitlab_vlci RealTime From 2000 Val\u00e8ncia City Council Contact Weather Forecast South Spain Valencia gitlab_vlci Daily From 2010 Val\u00e8ncia City Council Contact Bikeparking stands in Aarhus City Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality City of Aarhus Bike terminals in Aarhus, air and tools Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality Cykelterminal - Dataset Citybike locations in Aarhus Nordic Denmark GTM no specific value no specific value No Aarhus Municipality Aarhus Bycykel - Dataset Fast track bikeroutes in Aarhus Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality Supercykelsti i Aarhus Kommune - Dataset recreative bikeroutes in Aarhus Nordic Denmark GTM no specific value Ongoing No Aarhus Municipality Rekreative cykelruter - Dataset"},{"location":"data_catalog/instructions/","title":"How to add new datasets?","text":"<p>To add new datasets to the data catalog, an automated request system has been enabled through GitHub Issues and csv's files. </p> <p>Each of the datasets that you want to add to the data catalog must be attached as a different csv file. The template for this file can be found at the following link: Dataset Template.</p> <p></p>"},{"location":"data_catalog/instructions/#the-dataset-template","title":"The Dataset Template","text":"<p>The dataset template is a file in csv format that can be divided into two parts depending on its content. </p> <p>The first part collects general information about the dataset:</p> Description DATASET-NAME Dataset name. SUPER-NODE Name of the super node to which it belongs. TEF-NODE Name of the TEF node to which it belongs. SITE Name of the site to which it belongs. DATA-MODEL Link to the data model that complies. For example, the smart data model of firmware https://github.com/smart-data-models. SAMPLING-TIME Sampling time of the dataset (Realtime, X h/min/seg). HISTORICAL Historical data availability (From XXX/No). OWNER Owner of the dataset. GET-ACCESS Contact information to access the dataset. <p>The second part collects information about the fields (metadata) of the dataset:</p> Description ATTRIBUTE Variable name. TYPE Type of variable (Integer, Float, String, Boolean, etc.). UNITS (SI) Units of the variable in the International System. DESCRIPTION/COMMENTS Description/Explanation of the variable. <p>Warning</p> <ul> <li> <p>If you do not have information to complete a field, DO NOT delete it, LEAVE IT EMPTY.</p> </li> <li> <p>If a field is a web link, copy it in plain text, not as a hyperlink.</p> </li> </ul>"},{"location":"data_catalog/instructions/#example-a-filled-template","title":"Example: A filled template","text":"<p>The following image shows an example of a filled template:</p> <p></p>"},{"location":"data_catalog/instructions/#making-the-request-github-issues","title":"Making the request (GitHub Issues)","text":"<p>A GitHub account is required.</p> <p>To make the request to add new datasets to the data catalog, you must create an issue using the New dataset request template and follow the instructions.</p> <p>From the data catalog page and clicking on the <code>Add New Datasets</code> button you will directly access to adding a new issue.</p> <p></p> <p>In the Github Issues tab, click <code>Get started</code> in the New dataset request issue.</p> <p></p> <p>This will be the template that you must fill out.</p> <p></p>"},{"location":"data_catalog/instructions/#filling-the-issue","title":"Filling the issue","text":"<ol> <li>Add a Title: Only replace the <code>&lt;site_name&gt;</code> with the name of the site to which the dataset belongs (e.g. <code>Valencia</code>).</li> <li> <p>Attach files: Select the <code>&gt; Add files here.</code> line and insert the csv by clicking on the paper clip in the top bar. All the files you attach must appear one under the other within this Attached csv files section.</p> <p></p> </li> <li> <p>Comments: If you have any comments you can add them in the corresponding section.</p> </li> <li> <p>Submit new issue: Once everything is complete, publish the request.</p> <p></p> </li> </ol>"},{"location":"data_catalog/instructions/#reviewing-the-request","title":"Reviewing the request","text":"<p>Submitted the request, it will be processed automatically. Notifying in the same issue the start of the process and ending with the closing of the issue and the creation of a pull-request with the requested changes. </p> <p></p> <p>This pull request will be reviewed by the site administrators and, if everything is correct, it will be accepted and the information will be added to the data catalogue. If not, the user will be notified to make the necessary corrections.</p>"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/","title":"Super Node: Nordic | TEF Node: CitCom","text":""},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-bikeparking-stands-in-aarhus-city","title":"Site: GTM | Bikeparking stands in Aarhus City","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS ID number numerical identicator for each stand element txt the type of geographical element elementnavn txt name of the stand bemaerkninger txt comments beliggenhed txt adress og location oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_dato date person/system that corrected the item SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-bike-terminals-in-aarhus-air-and-tools","title":"Site: GTM | Bike terminals in Aarhus, air and tools","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS fid txt identicator for each route type txt type of terminal beskrivelse txt description adress txt adress by txt city komnr number municipality number oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_af internal ID person/system that corrected the item rettet_dato date date of correction MI_STYLE MI_PRINX number SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-citybike-locations-in-aarhus","title":"Site: GTM | Citybike locations in Aarhus","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS _id number identicator bid number place txt name og location alttext txt comments lat number degrees of latitude ing number degrees of longitude date date"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-fast-track-bikeroutes-in-aarhus","title":"Site: GTM | Fast track bikeroutes in Aarhus","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS fid txt identicator for each route navn_cykelrute txt Name of route komnr number municipality number navn_sted txt name of place bemaerkninger txt oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_af internal ID person/system that corrected the item rettet_dato date date of correction MI_STYLE MI_PRINX number SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/nordic_citcom_gtm/#site-gtm-recreative-bikeroutes-in-aarhus","title":"Site: GTM | recreative bikeroutes in Aarhus","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS fid txt identicator for each route straekning number nummerical identicator for each route oprettet_af internal ID person/system that created the item oprettet_dato date date of creation rettet_dato date person/system that corrected the item MI_STYLE MI_PRINX SP_geometry coordinates geograpical coordinates"},{"location":"data_catalog/metadata_datasets/south_spain_valencia/","title":"Super Node: South | TEF Node: Spain","text":""},{"location":"data_catalog/metadata_datasets/south_spain_valencia/#site-valencia-waste-container","title":"Site: Valencia | Waste Container","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS location geo:json Geojson reference to the element. Can be Point, LineString, Polygon, MultiPoint, MultiLineString or MultiPolygon address string Civil address where the container is located. project string Project to which this entity belongs. category list Container category(s). storedWasteKind string Type of waste stored. depth number m Container depth height number m Container height. fillingLevel number Percentage of container filling in parts by 1. temeprature number \u00baC Temperature inside the container. c_factor number Multiplicative factor between [0 - 2] to adapt the measurement taken by the sensor with the visual % of filling that the client has of it."},{"location":"data_catalog/metadata_datasets/south_spain_valencia/#site-valencia-weather-forecast","title":"Site: Valencia | Weather Forecast","text":"ATTRIBUTE TYPE UNITS (SI) DESCRIPTION/COMMENTS name string The name of where the weather forecast is located. location geo:json Geojson reference to the element. It will be a Point. dateIssued date-time ISO8601 UTC Date and time of issuance of the forecast in ISO8601 UTC format. validFrom date-time ISO8601 UTC Date and time of start of the validity period in ISO8601 UTC format validTo date-time ISO8601 UTC Date and time of end of the validity period in ISO8601 UTC format. temperatureSurface number \u00baC Surface temperature. windSpeed number m/s Wind speed at 10 meters high dewPoint2m number \u00baC Dew point at 2 meters high."},{"location":"documentation/","title":"Documentation","text":"<p>Reports and deployment guides of different components.</p>"},{"location":"documentation/#how-can-i-connect-different-data-platforms-or-data-spaces","title":"How can I connect different data platforms or data spaces?","text":"<ul> <li> <p> Data Federation</p> <p>The data federation section groups the guides to be able to communicate different brokers based on their technology.</p> <p> Learn more</p> </li> </ul>"},{"location":"documentation/#what-data-space-connector-technologies-exist-and-how-can-i-deploy-a-connector","title":"What data space connector technologies exist and how can I deploy a connector?","text":"<ul> <li> <p> Data Space Connector</p> <p>The Data Space Connectors section groups the guides of data space connector technology.</p> <p> Learn more</p> </li> </ul>"},{"location":"documentation/data_federation/","title":"Data Federation","text":"<p>A key aspect of data spaces is their interoperability, which essentially refers to the union of several independent data spaces under a common set of rules and protocols to enable data exchange. This federation allows data to be shared and accessed securely and efficiently among different organizations, while at the same time maintaining the sovereignty and control of the data by the entities that own them.</p> <p>The federation arises in data spaces due to the need for collaboration and large-scale data sharing in today's digital world. The challenges associated with managing large volumes of data, protecting privacy and security, and the need to extract value from data have led to the creation of federated data spaces. These spaces allow organizations to work together to leverage data more effectively, while respecting regulations and the rights of data owners.</p> Brokers communication NGSI-LD NGSI-v2 Custom NGSI-LD not tested Lepus | IoT Agent not tested NGSI-v2 Lepus | IoT Agent not in data spaces not tested Custom not tested not tested not tested"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/","title":"FIWARE IoT-Agent","text":"<p>Repository </p> <p>This section explains how to connect an NGSI-V2 broker with an NGSI-LD broker (Fiware Orion) through subscriptions to a FIWARE IoT-Agent. This connection between brokers arises as an alternative to the use of FIWARE Lepus due to the lack of testing and the experimental nature of this service.</p> <p></p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#simulation-environment","title":"Simulation Environment","text":"<p>An environment has been created with docker compose that deploys in an isolated way (on two independent networks) the architectures FIWARE NGSI-V2 and FIWARE NGSI-LD.</p> <p></p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#services","title":"Services","text":"<p>Grouping the services according to the network or networks they belong to, we have:</p> <ul> <li>Network NGSI-V2 <code>ngsiv2_network</code><ul> <li>IoT-Agent <code>ngsiv2.iotagent</code></li> <li>Mongo DB <code>ngsiv2.mongodb</code></li> <li>Orion-V2 <code>ngsiv2.orionv2</code></li> </ul> </li> <li>Network NGSI-LD <code>ngsild_network</code><ul> <li>IoT-Agent <code>ngsild.iotagent</code></li> <li>Mongo DB <code>ngsild.mongodb</code></li> <li>LD Context <code>ngsild.context</code></li> <li>Orion-LD <code>ngsild.orionld</code></li> <li>Quantum Leap <code>ngsild.quantumleap</code>: Fiware data persistence service</li> <li>CreateDB <code>ngsild.cratedb</code>: DB for Quantum Leap.</li> </ul> </li> <li>Both networks <code>ngsiv2_network</code> y <code>ngsild_network</code><ul> <li>Jupyter Server</li> <li>NGINX</li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#before-continuing","title":"Before continuing...","text":""},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#services-checks","title":"Services checks","text":""},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#mongodb-connection","title":"MongoDB connection","text":"<p>A quick way to check that the environment is correctly deployed and the different services communicate with each other is to verify the different databases and collections through MongoDB Compass:</p> MongoDB Compass <ul> <li> <p>MongoDB-V2: localhost, port <code>27028</code></p> <ul> <li>IoT-Agent<ul> <li> DB: <code>iotagent-json</code><ul> <li> Colection: <code>groups</code></li> </ul> </li> </ul> </li> <li>Context Broker: The database will appear when it has some entity.</li> </ul> </li> <li> <p>MongoDB-LD: localhost, port <code>27027</code></p> <ul> <li>IoT-Agent<ul> <li> DB: <code>iotagent-json</code><ul> <li> Colection: <code>groups</code></li> </ul> </li> </ul> </li> <li>Context Broker<ul> <li> DB: <code>orionld</code><ul> <li> Colection: <code>contexts</code></li> </ul> </li> <li> DB: <code>myOrion</code><ul> <li> Colection <code>entities</code></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#communication-with-the-iot-agents","title":"Communication with the IoT-Agents","text":"<p>By requesting the version from the IoT-Agents, it can be easily verified that they are correctly deployed and that they are visible among the different networks of the environment.</p> From <code>jupyter-server</code> <p>IoT-Agent NGSI-V2:</p> <pre><code>curl -X GET 'http://ngsiv2.iotagent:4041/iot/about' | jq .\n</code></pre> <p>IoT-Agent NGSI-LD:</p> <pre><code>curl -X GET 'http://ngsild.iotagent:4041/iot/about' | jq .\n</code></pre> From <code>fiware-orionv2</code> to the IoT-Agent LD <pre><code>curl -X GET 'http://ngsiv2.to-ngsild:4041/iot/about' | jq .\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#terminal-access-to-the-containers","title":"Terminal access to the containers","text":"<p>To access the containers from the terminal, the command <code>docker exec</code> can be used.</p> <pre><code>docker exec -it -u &lt;usuario&gt; &lt;nombre_contenedor&gt; &lt;sh/bash&gt;\n</code></pre> <p>Example</p> <code>jupyter-server</code><code>fiware-orionv2</code><code>fiware-orionld</code> <pre><code>docker exec -it orion-jupyter bash\n</code></pre> <ul> <li>Access (by default): <code>root</code></li> <li>Package installer: <code>apk</code></li> </ul> <pre><code>docker exec -it -u root fiware-orionv2 bash\n</code></pre> <ul> <li>Access (by default): <code>nobody</code></li> <li>Package installer: <code>apt</code></li> </ul> <pre><code>docker exec -it fiware-orionld bash\n</code></pre> <ul> <li>Access (by default): <code>root</code></li> <li>Package installer: <code>yum</code></li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#steps-for-the-connection-between-brokers","title":"Steps for the connection between brokers","text":"<p>In theory, the procedure to connect two brokers through the IoT-Agent will be as follows:</p> <ol> <li> <p>Provisioning of a service group (IoT-Agent)</p> </li> <li> <p>Devices creation (IoT-Agent)</p> </li> <li> <p>Subscription (Context Broker)</p> </li> <li> <p>Modification of devices values (IoT-Agent)</p> </li> </ol> <pre><code>graph LR\n    fir(1\u00ba - Provisioning\\n of a service group) -.-&gt; A;\n    sec(2\u00ba - Provisioning\\n of a service group) -.-&gt; C;\n    thi(3\u00ba - Custom\\n subscription V2 to LD) -.-&gt; B;\n    four(4\u00ba - Modification\\n of devices values) -.-&gt; A;\n    subgraph NGSI-V2\n        A[IoT-Agent];\n        A --- B[Orion Broker];\n    end\n    subgraph NGSI-LD\n        B --&gt; C[IoT-Agent\\n];\n        C --- D[Orion Broker];\n    end</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#simulations","title":"Simulations","text":""},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#non-normalized-data-according-to-data-models","title":"Non-normalized data according to data-models","text":"<p>The objective of this example is to show the procedure to connect two brokers, setting aside the use of data-models as a normalizing element.</p> Property Value Type <code>example-type</code> fiware-service <code>vrainIoTA</code> <p>All the code shown is designed to be run from the <code>jupyter-server container</code></p> <pre><code>docker exec -it orion-jupyter bash\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#provisioning-of-a-group-of-services-iot-agent","title":"Provisioning of a group of services (IoT-Agent)","text":"<p>Creation of a service in the IoT-Agent with the attributes that all devices belonging to this service will have.</p> <p>Through services, sets of devices can be grouped together and the creation of these can be automated.</p> Editable fields (in both versions) <ul> <li>URL: http://ngsiv2.iotagent:4041/iot/services</li> <li>Headers:<ul> <li>fiware-service: vrainIoTA</li> <li>fiware-servicepath: /</li> </ul> </li> <li>apikey: vrain2gpepnvsb2uv4s40d59ov (value random, in principle the main requirement is to maintain the length and characters).</li> <li>cbroker: http://ngsiv2.orion:1026</li> <li>entity_type: example-type</li> <li>attributes: Modify according to the attributes of the devices. The structure of: <code>object_id</code>, <code>name</code> and <code>type</code> must be maintained..</li> </ul> cURL-V2cURL-LD <pre><code>curl -iX POST \\\n'http://ngsiv2.iotagent:4041/iot/services' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"services\": [\n        {\n            \"apikey\":      \"vrain2gpepnvsb2uv4s40d59ov\",\n            \"cbroker\":     \"http://ngsiv2.orion:1026\",\n            \"entity_type\": \"example-type\",\n            \"resource\":    \"/iot/json\",\n            \"attributes\": [\n                {\n                    \"object_id\": \"t\",\n                    \"name\": \"temperature\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"h\",\n                    \"name\": \"humidity\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"f\",\n                    \"name\": \"fillingLevel\",\n                    \"type\": \"Number\"\n                }\n            ]\n        }\n    ]\n}'\n</code></pre> <pre><code>curl -iX POST \\\n'http://ngsild.iotagent:4041/iot/services' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"services\": [\n        {\n            \"apikey\":      \"vrainld2gpnvsb2uv4s40d59ov\",\n            \"cbroker\":     \"http://ngsild.orion:1026\",\n            \"entity_type\": \"example-type\",\n            \"resource\":    \"/iot/json\",\n            \"attributes\": [\n                {\n                    \"object_id\": \"t\",\n                    \"name\": \"temperature\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"h\",\n                    \"name\": \"humidity\",\n                    \"type\": \"Number\"\n                },\n                {\n                    \"object_id\": \"f\",\n                    \"name\": \"fillingLevel\",\n                    \"type\": \"Number\"\n                }\n            ]\n        }\n    ]\n}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 201 Created\nX-Powered-By: Express\nFiware-Correlator: c3e07e1c-62be-4ae4-8ee9-f112f585c077\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\nDate: Wed, 25 Oct 2023 11:39:01 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n</code></pre> <p>Result in MongoDB</p> <p>It should appear within the <code>iotagent-json</code> database, a collection <code>groups</code> and within this as many documents as services have been created.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#checks","title":"Checks","text":"<p>The service groups can be obtained with the following request:</p> cURL-V2cURL-LD <pre><code>curl -X GET 'http://ngsiv2.iotagent:4041/iot/services' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' | jq .\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>{\n\"count\": 1,\n\"services\": [\n    {\n    \"commands\": [],\n    \"lazy\": [],\n    \"attributes\": [\n        {\n        \"object_id\": \"t\",\n        \"name\": \"temperature\",\n        \"type\": \"Number\"\n        },\n        {\n        \"object_id\": \"h\",\n        \"name\": \"humidity\",\n        \"type\": \"Number\"\n        },\n        {\n        \"object_id\": \"f\",\n        \"name\": \"fillingLevel\",\n        \"type\": \"Number\"\n        }\n    ],\n    \"_id\": \"653900b475ac594b1cc9e607\",\n    \"resource\": \"/iot/json\",\n    \"apikey\": \"vrain2gpepnvsb2uv4s40d59ov\",\n    \"service\": \"vrainiota\",\n    \"subservice\": \"/\",\n    \"__v\": 0,\n    \"static_attributes\": [],\n    \"internal_attributes\": [],\n    \"entity_type\": \"example-type\"\n    }\n]\n}\n</code></pre> <pre><code>curl -X GET 'http://ngsild.iotagent:4041/iot/services' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' | jq .\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>{\n\"count\": 1,\n\"services\": [\n    {\n    \"commands\": [],\n    \"lazy\": [],\n    \"attributes\": [\n        {\n        \"object_id\": \"t\",\n        \"name\": \"temperature\",\n        \"type\": \"Number\"\n        },\n        {\n        \"object_id\": \"h\",\n        \"name\": \"humidity\",\n        \"type\": \"Number\"\n        },\n        {\n        \"object_id\": \"f\",\n        \"name\": \"fillingLevel\",\n        \"type\": \"Number\"\n        }\n    ],\n    \"_id\": \"6540cb26101b6eba30d4d653\",\n    \"resource\": \"/iot/json\",\n    \"apikey\": \"vrainld2gpnvsb2uv4s40d59ov\",\n    \"service\": \"vrainiota\",\n    \"subservice\": \"/\",\n    \"__v\": 0,\n    \"static_attributes\": [],\n    \"internal_attributes\": [],\n    \"entity_type\": \"example-type\"\n    }\n]\n}\n</code></pre> <p>Checklist - Before continuing...</p> <p>To continue correctly with the example, you must have:</p> <ul> <li> IoT-Agent V2: Having a service created.<ul> <li>Take note (to use later) of the service's:<ul> <li>Attributes.</li> <li><code>apikey</code></li> </ul> </li> </ul> </li> <li> IoT-Agent LD: Having a service created.<ul> <li>Take note (to use later) of the service's:<ul> <li>Attributes.</li> <li>`apikey</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#aprovisionamiento-de-dispositivos-iot-agent","title":"Aprovisionamiento de dispositivos (IoT-Agent)","text":"<p>The creation of devices can be done in two ways:</p> <ul> <li>Manual: By explicitly creating a device.</li> <li>Automated: By using a service (previously created) and modifying the values of a device from the service.</li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#manual-method","title":"Manual Method","text":"<p>Info</p> <p>This section is not necessary for the correct operation of the example. It will only be necessary to know the automated method to use it after creating the subscription and verifying that it works.</p> <p>A specific device is added to the created service:</p> cURL-V2 <pre><code>curl -iX POST \\\n'http://ngsiv2.iotagent:4041/iot/devices' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"devices\": [\n        {\n            \"device_id\":   \"example001\",\n            \"entity_name\": \"example:example001\",\n            \"entity_type\": \"example-type\",\n            \"timezone\":    \"Europe/Madrid\"\n        }\n    ]\n}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 201 Created\nX-Powered-By: Express\nFiware-Correlator: 421f4048-f56d-423c-8868-088ae96965bf\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\nDate: Wed, 25 Oct 2023 12:10:10 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n</code></pre> <p>This process only adds a device in the IoT-Agent but does not modify the value of the attributes. To modify the device values, you must use the request of the automated method.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#automated-method-through-services","title":"Automated method (through services)","text":"<p>This method fulfills two functions:</p> <ul> <li>Create a device (if it does not exist).</li> <li>Modify the values of its attributes.</li> </ul> URL Structure <p>Replace the values of: <code>&lt;iotagent_adress&gt;</code>, <code>&lt;apikey&gt;</code> y <code>&lt;device_id&gt;</code> for the corresponding values.</p> <pre><code>http://&lt;iotagent_adress&gt;:7896/iot/json?k=&lt;apikey&gt;&amp;i=&lt;device_id&gt;\n</code></pre> <p>Warning</p> <p>The port for this request changes from <code>4041</code> to <code>7896</code>.</p> cURL-V2cURL-LD <pre><code>curl -iX POST \\\n'http://ngsiv2.iotagent:7896/iot/json?k=vrain2gpepnvsb2uv4s40d59ov&amp;i=example001' \\\n-H 'Content-Type: application/json' \\\n-d '{\"t\": 1, \"h\":20, \"f\": 30}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 200 OK\nX-Powered-By: Express\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\nDate: Wed, 25 Oct 2023 12:15:01 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n</code></pre> <p>It does not work with versions higher than <code>2.3.0</code> of the IoT-Agent.</p> <pre><code>curl -iX POST \\\n'http://ngsiv2.to-ngsild:7896/iot/json?k=vrainld2gpnvsb2uv4s40d59ov&amp;i=example001' \\\n-H 'Content-Type: application/json' \\\n-d '{\"t\": 1, \"h\":20, \"f\": 30}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 200 OK\nServer: nginx/1.25.2\nDate: Thu, 02 Nov 2023 11:35:07 GMT\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nConnection: keep-alive\nX-Powered-By: Express\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\n</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#checks_1","title":"Checks","text":"<p>To check the creation and modification of the values of a device, it can be done directly through MongoDB.</p> <p>Results in MongoDB</p> cURL-V2cURL-LD <p>As a result of this operation, a new database <code>orion-&lt;fiware-service&gt;</code> (orion-vrainiota) is created in mongodb with a <code>entities</code> collection and within this a document per device.</p> <p>As a result of this operation, a new database <code>myOrion-&lt;fiware-service&gt;</code> (myOrion-vrainiota) is created in mongodb with a <code>entities</code> collection and within this a document per device.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#subscription-of-orion-context-broker-v2-to-the-iot-agent-ld","title":"Subscription of Orion Context Broker V2 to the IoT-Agent LD","text":"<p>Finally, a custom subscription is created in the V2 broker, in this way, for each modification of the values of a device, the broker will send a POST request (just like the previous one) in an automated way.</p> cURL (BrokerV2 -&gt; IoT-Agent LD) <p>In the httpCustom:url, you cannot put addresses with <code>-</code></p> <pre><code>curl -iX POST \\\n'http://ngsiv2.orion:1026/v2/subscriptions' \\\n-H 'Content-Type: application/json' \\\n-H 'fiware-service: vrainIoTA' \\\n-H 'fiware-servicepath: /' \\\n-d '{\n    \"description\": \"Reenv\u00edo datos entre brokers.\",\n    \"status\": \"active\",\n    \"subject\": {\n        \"entities\": [\n            {\n                \"idPattern\": \".*\",\n                \"type\": \"example-type\"\n            }\n        ],\n        \"condition\": {\n            \"attrs\": [],\n            \"notifyOnMetadataChange\": true\n        }\n    },\n    \"notification\": {\n        \"attrs\": [],\n        \"onlyChangedAttrs\": false,\n        \"attrsFormat\": \"normalized\",\n        \"httpCustom\": {\n            \"url\": \"http://ngsiv2.to-ngsild:7896/iot/json\",\n            \"method\": \"POST\",\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            },\n            \"qs\": {\n                \"i\": \"${id}\",\n                \"k\": \"vrainld2gpnvsb2uv4s40d59ov\"\n            },\n            \"json\": {\n                \"t\": \"${temperature}\",\n                \"h\": \"${humidity}\",\n                \"f\": \"${fillingLevel}\"\n            }\n        }\n    }\n}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 201 Created\nDate: Thu, 02 Nov 2023 11:56:13 GMT\nFiware-Correlator: d244b742-7976-11ee-8fe6-0242ac130005\nLocation: /v2/subscriptions/65438e5d0a524d42d10bffca\nContent-Length: 0\n</code></pre> <p>Analyzing in detail some elements of this request we have:</p> <ul> <li>URL: Points to the subscriptions of the V2 broker.</li> <li>fiware-service: Necessary for the subscription to work.</li> <li>fiware-servicepath: Necessary for the subscription to work.</li> <li>notification<ul> <li>attrs: There is no need to specify attributes as they are later selected in the body of the request.</li> <li>httpCustom<ul> <li>url: Points to the IoT-Agent LD. Since the V2 broker network does not have direct visibility to the LD network, this address points to the reverse proxy between both networks.</li> <li>qs: <ul> <li>k: The <code>k</code> parameter refers to the <code>apiKey</code> of the service in IoT-Agent of LD.</li> </ul> </li> <li>json: This is the body of the request. It relates the <code>id</code> of the attributes in the IoT-Agent LD (<code>t</code>, <code>h</code> and <code>f</code>) with the attributes in the V2 broker (<code>${temperature}</code>, <code>${humidity}</code> and <code>${fillingLevel}</code>).</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#checks_2","title":"Checks","text":"<p>To verify that the subscription is working, you simply need to modify the value of a device using the IoT-Agent or directly modify an attribute of an entity in the V2 broker. In this case, the first option is chosen through the IoT-Agent.</p> <p>Note how the request points to the address of the IoT-Agent in V2.</p> <pre><code>curl -iX POST \\\n'http://ngsiv2.iotagent:7896/iot/json?k=vrain2gpepnvsb2uv4s40d59ov&amp;i=example001' \\\n-H 'Content-Type: application/json' \\\n-d '{\"t\": 656, \"h\":87, \"f\": \"40\"}'\n</code></pre> If everything goes well, we will receive a response like this: <pre><code>HTTP/1.1 200 OK\nX-Powered-By: Express\nContent-Type: application/json; charset=utf-8\nContent-Length: 2\nETag: W/\"2-vyGp6PvFo4RvsFtPoIWeCReyIC8\"\nDate: Thu, 02 Nov 2023 12:31:00 GMT\nConnection: keep-alive\nKeep-Alive: timeout=5\n</code></pre> <p>After executing this request, we will observe how in MongoDB (both V2 and LD) the values of the device in question (<code>example001</code>) appear or are modified.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/iot_agent/#references","title":"References","text":"<p>FIWARE IoT-Agent - Github Each version (V2 or LD) is located in a different branch.</p> <p>FIWARE Docu. - API Custom Notificacions</p> <p>FIWARE Docu. - API httpCustom</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/","title":"Lepus","text":"<p>Repository </p> <p>FIWARE-Lepus is an NGSI-LD wrapper for use with NGSI-v2 Context Brokers. It understands the NGSI-LD endpoints and inputs, converts them to NGSI-v2, makes a request to the NGSI-v2 broker behind it and transforms responses back to NGSI-LD using a fixed JSON-LD @context. It supports the NGSI-LD federationOps endpoints only and is designed to be used as a registered source with NGSI-LD Context Brokers in federation mode.</p> <pre><code>graph LR\n  A[NGSI-v2 Context Broker] --- B[Lepus];\n  B ---|NGSI-LD| C[Client];</code></pre>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/#getting-started","title":"Getting started","text":"<p>Warning</p> <p>Lepus is a library still in development. It is considered untested and experimental. Errors may occur.</p> <p>Create a <code>docker-compose.yml</code> file with the following config. Please, replace <code>NGSI_V2_CONTEXT_BROKER</code> with the URL / IP address of your NGSI-v2 Context Broker. Also set <code>CONTEXT_URL</code> to the desired context (for instance WasteManagement context). </p> <pre><code>version: '3.9'\nservices:\n  lepus:\n    image: quay.io/fiware/lepus\n    build:\n      context: .\n      dockerfile: Dockerfile\n    hostname: adapter\n    container_name: lepus\n    networks:\n      - default\n    expose:\n      - \"3000\"\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DEBUG=adapter:*\n      - NGSI_V2_CONTEXT_BROKER=http://orion2:1026/v2\n      - CONTEXT_URL=https://fiware.github.io/tutorials.Step-by-Step/tutorials-context.jsonld\n      - NOTIFICATION_RELAY_URL=http://adapter:3000/notify\n      - NODE_TLS_REJECT_UNAUTHORIZED=0\nnetworks:\n  default:\n</code></pre> <p>Create and start the container by using the following command:</p> <pre><code> docker compose up\n</code></pre> <p>Set-up your AI service / client so it points to <code>127.0.0.1.1:3000</code>.</p>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/#debugging","title":"Debugging","text":"<p>As mentioned above, Lepus is still in its early stages, so it lacks some features, and wild bugs may appear. In order to make it easier to detect and report errors, a developing / debugging repository was created. If you find any issues or want to request a feature, please, open a new issue in the official repository.</p> <p>Steps to run the development version (it is necessary to have Node.js installed):</p> <ol> <li> <p>Clone the repository and navigate to its root folder: <pre><code>git clone https://github.com/CitCom-VRAIN/lepus-dev &amp;&amp; cd lepus-dev\n</code></pre></p> </li> <li> <p>Install dependencies <pre><code>npm install\n</code></pre></p> </li> <li> <p>Run <pre><code>npm start\n</code></pre></p> </li> </ol>"},{"location":"documentation/data_federation/ngsiv2_to_ld/lepus/#track-and-status-of-detected-errors","title":"Track and status of detected errors","text":"<ul> <li> Missing <code>NGSILD-Tenant header</code> (equivalent to <code>Fiware-Service</code>)   </li> <li> Missing <code>scopeQ parameter</code> (equivalent to <code>Fiware-ServicePath</code>)  </li> <li> Default response timeout too low for the VLCi platform   <ul> <li>Use <code>NGSI_V2_TIMEOUT</code> environment variable to set a custom value (1000ms by default)</li> </ul> </li> <li> Missing @context property when requesting entities </li> <li> HTTPS / SSL problems  <ul> <li>Lepus' protocol is http. It should be fine for working locally and testing purposes</li> <li><code>NODE_TLS_REJECT_UNAUTHORIZED=0</code> must be set to avoid SSL certificate checking problem</li> </ul> </li> <li> Static context</li> </ul>"},{"location":"documentation/data_space_connectors/","title":"Overview","text":"<ul> <li> <p> Eclipse</p> <p>Eclipse data space connector.</p> <p> Learn more</p> </li> <li> <p> Fiware</p> <p>Fiware data space connector.</p> <p> Learn more</p> </li> </ul>"},{"location":"documentation/data_space_connectors/#status-overview-of-existing-connectors","title":"Status overview of existing connectors","text":"<p>The following list has been extracted from Section 2.1 of the IDSA Data Connector Report (January 2024). Only open source connectors have been considered:</p> Name of connector Real-time data MIM 1 ready MIM 2 ready Deployment guide - Quality grade Comments Eclipse Dataspace Connector (EDC Connector) \u2713 \u2715 \u2713 A EdgeDS Connector - - - - Based on IDSA DSC, which is currently no longer maintained but looking for new maintainers. EGI Datahub Connector Pending review \u2715 \u2713 Pending review FIWARE Data Space Connector \u2713 \u2713 \u2713 C At this moment, deployment guides are only available on AWS with Openshift. Poor documentation. High computational requirements on AWS with Openshift. GATE Dataspace Connector - - - - Based on IDSA DSC, which is currently no longer maintained but looking for new maintainers OneNet Connector Pending review Pending review Pending review Pending review Source Code will be available in GitHub upon project completion..Based on TRUE connector. Claims to be ready-to-go, ready to be installed in any environment and integrated with existing platforms via APIs. Fully integrated with the FIWARE Context Broker (in the NGSI-LD version) Silicon Economy EDC Pending review Pending review Pending review Pending review A reference implementation of the Connector from the Eclipse Dataspace Components (EDC) for Silicon Economy projects. <p>Info</p> <p>More information about Data Spaces here.</p>"},{"location":"documentation/data_space_connectors/eclipse_mvd/","title":"Eclipse Connector","text":""},{"location":"documentation/data_space_connectors/eclipse_mvd/#introduction","title":"Introduction","text":"<p>The Eclipse Dataspace Components (EDC) provide a framework for sovereign, inter-organizational data sharing. They implement the IDS Dataspace Protocol (DSP) as well as relevant protocols associated with GAIA-X. The EDC are designed in an extensible way in order to support alternative protocols and integrate in various ecosystems. </p> <p>Eclipse also offers the Minimum Viable Dataspace (MVD), a sample implementation of a dataspace that leverages the Eclipse Dataspace Components (EDC). The main purpose is to demonstrate the capabilities of the EDC, make dataspace concepts tangible based on a specific implementation, and to serve as a starting point to implement a custom dataspace.</p> <p>Below is a step-by-step description of deploying this example locally, and some first impressions are documented.</p>"},{"location":"documentation/data_space_connectors/eclipse_mvd/#getting-started","title":"Getting started","text":"<p>The example shows three companies forming a data space through their corresponding connectors. In addition, each company has access to a web dashboard where, among other things, they can see a catalog of the data offered by the other companies.  </p> <p>The dashboard lets you \"negotiate contracts\" to access the data offered by the other companies. Once a contract has been negotiated, the data can be downloaded.</p> <p>The data exchanged are files stored in Azurite (a local Azure instance, just like Amazon's buckets). In other words, the Eclipse Minimum Viable Data space essentially provides a controlled environment for file exchange. </p>"},{"location":"documentation/data_space_connectors/eclipse_mvd/#set-up","title":"Set-up","text":"<ol> <li> <p>First, make sure that Java 17 is installed in your system. If you are on Ubuntu / Debian you can install it by: <pre><code>sudo apt install openjdk-17-jdk\n</code></pre></p> </li> <li> <p>Once your system is ready, clone the web dashboard: <pre><code>git clone https://github.com/eclipse-edc/DataDashboard\n</code></pre></p> </li> <li> <p>Then set the environment variable <code>MVD_UI_PATH</code> to the path of the cloned DataDashboard repository: <pre><code>export MVD_UI_PATH=\"/path/to/mvd-datadashboard\"\n</code></pre></p> </li> <li> <p>Now, clone the MVD repository and navigate to its root folder: <pre><code>git clone https://github.com/eclipse-edc/MinimumViableDataspace &amp;&amp; cd MinimumViableDataspace\n</code></pre></p> </li> <li>Build the MVD <pre><code>./gradlew build -x test\n</code></pre></li> <li>Build the <code>EDC Connector</code> and <code>RegistrationService</code> runtimes. As we are running MVD locally, we include <code>useFsVault</code> to indicate that the system will be using the local file-system based key vault. Execute the following command to build the connector JAR and registration service JAR: <pre><code>./gradlew -DuseFsVault=\"true\" :launchers:connector:shadowJar\n./gradlew -DuseFsVault=\"true\" :launchers:registrationservice:shadowJar\n</code></pre></li> <li> <p>Then, to bring up the dataspace with the <code>ui</code> profile, please execute the following command from the MVD root folder: <pre><code>docker compose --profile ui -f system-tests/docker-compose.yml up --build\n</code></pre></p> </li> <li> <p>Set the environment variable `TEST_ENVIRONMENT`` to local to enable local blob transfer test and then run MVD system test using the following command: <pre><code>export TEST_ENVIRONMENT=local\n./gradlew :system-tests:test -DincludeTags=\"ComponentTest,EndToEndTest\"\n</code></pre></p> </li> </ol>"},{"location":"documentation/data_space_connectors/eclipse_mvd/#file-transfer-demo","title":"File transfer demo","text":"<ol> <li> <p>Download and install Microsoft Azure Storage Explorer to connect to the <code>Azurite</code> storage container. It will let us view the transferred files.</p> </li> <li> <p>Using Microsoft Azure Storage Explorer, connect to the local blob storage account on <code>localhost:10000</code> (provided by Azurite) of <code>company1</code>:</p> <ul> <li>Account name: <code>company1assets</code></li> <li>Password: <code>key1</code></li> </ul> </li> <li> <p>Create a container named <code>src-container</code>.</p> </li> <li>Upload a dummy <code>text-document.text</code> file into the newly created container.</li> <li>The following steps initiate and complete a file transfer with the provided test document:<ol> <li>Open the website of company1 (e.g. http://localhost:7080) and verify the existence of two assets in the   section <code>Assets</code>.</li> <li>Open the website of the company2 (e.g. http://localhost:7081) and verify six existing assets from all participants in   the <code>Catalog Browser</code>.<ul> <li>In the <code>Catalog Browser</code> click <code>Negotiate</code> for the asset <code>test-document_company1</code>.<ul> <li>There should be a message <code>Contract Negotiation complete! Show me!</code> in less than a minute.</li> </ul> </li> </ul> </li> <li>From the previous message click <code>Show me!</code>. If you missed it, switch manually to the section <code>Contracts</code>.<ul> <li>There should be a new contract. Click <code>Transfer</code> to initiate the transfer process.</li> <li>A dialog should open. Here, select as destination <code>AzureStorage</code> and click <code>Start transfer</code>.</li> <li>There should be a message <code>Transfer [id] complete! Show me!</code> in less than a minute. (Where <code>id</code> is a UUID.)</li> </ul> </li> <li>To verify the successful transfer the Storage Explorer can be used to look into the storage account of <code>company2</code>.<ul> <li>Storage account name and key is set in <code>system-tests/docker-compose.yml</code> for the service <code>azurite</code>. Default name   is <code>company2assets</code>, key is <code>key2</code>.</li> <li>There should be new container in the storage account containing two files <code>.complete</code> and <code>text-document.txt</code>.</li> </ul> </li> </ol> </li> </ol>"},{"location":"documentation/data_space_connectors/fiware/","title":"FIWARE","text":"<p>Fiware is embarking on the development of different services that in combination meet the requirements defined for implementing a data space. For these developments they are following the DSBA Technical Convergence recomentations.</p> <p>This is a technology that Fiware currently has in development, so it is open to major modifications. Currently, versions 2.x.x are the most advanced, achieving significant improvements in simplicity and ease of use at the local level compared to versions 1.x.x.</p>"},{"location":"documentation/data_space_connectors/fiware/#legacy-version-1xx","title":"Legacy version (1.x.x)","text":""},{"location":"documentation/data_space_connectors/fiware/#demo-setup-dsba-compliant","title":"Demo-Setup DSBA-compliant","text":"<p>As a first approximation of a future data space, Fiware developed a fairly complete example (Demo-Setup DSBA-compliant Dataspace), which tries to address the main blocks of a data space:</p> <ul> <li>Data Space Operator (Trust Anchor)</li> <li>Marketplace</li> <li>Data Space Connector (Provider or Consumer)</li> </ul> <p></p> <p>However, this example is a demo to show the feasibility of the technology, but it is far from being easily reusable in other real environments or putting it into production systems. Furthermore, it is designed to be deployed in Red Hat Openshift AWS, so the cost of maintaining it may be a limiting factor for a simple test of the technology.</p>"},{"location":"documentation/data_space_connectors/fiware/#minimum-data-space","title":"Minimum Data Space","text":"<p>As an alternative to the previous Fiware example, a minimum data space architecture is proposed, which consists of:</p> <ul> <li>Trusted Participant/Issuers Registry (fulfilling the functions of Trust Anchor). </li> <li>FIWARE Data Space Connector (at least two).</li> </ul> <p>This architecture is what we could call the Minimum Data Space.</p> <p></p>"},{"location":"documentation/data_space_connectors/fiware/#guidelines-for-deployments","title":"Guidelines for deployments","text":"<p>Guidelines</p>"},{"location":"documentation/data_space_connectors/fiware/#latest-version-2xx","title":"Latest version (2.x.x)","text":"<ul> <li>New version</li> </ul>"},{"location":"documentation/data_space_connectors/fiware/latest_2.X.X/","title":"Latest version 2.X.X","text":"<p>Warning</p> <p>This guide is still a work in progress. Errors may appear.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/","title":"Legacy version 1.X.X","text":"<p>As explained earlier, the legacy version of the data space was composed of: the Operator, the Marketplace, and the Connector. Here, their architectures are detailed.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/#architectures","title":"Architectures","text":""},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/#data-space-operator","title":"Data Space Operator","text":"<p>The Dataspace Operator acts as the Trust Anchor of the Data Space. It provides the particpants information on whom to trust.</p> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/#minimal-operator-trust-anchor","title":"Minimal Operator (Trust Anchor)","text":"<p>For a minimal trust anchor, the Trusted Participants Registry (or Trusted Issuers Registry) is the unique service that is necessary. This can be managed through its API.</p> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/#marketplace","title":"Marketplace","text":"<p>Warning</p> <p>This architecture has been derived from the different Helm packages that form the marketplace, so there may be some differences or connections between services that are not reflected.</p> <p>The marketplace that Fiware proposes in this example is composed of the following services:</p> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/#fiware-data-space-connector","title":"Fiware Data Space Connector","text":"<p>Fiware Data Space Connector is a component that allows the data owner to share data with other participants in a secure and controlled way. </p> <p>A more extensive documentation about the connector and the supported flows in a data space it supports can be found at the FIWARE data-space-connector repository.</p> <p>To deploy the Data Space Connector Fiware provides this repository.</p> <p>This component is composed of the following services:</p> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/#deployments","title":"Deployments","text":"<ul> <li>AWS Deployment</li> <li>Local Deployment</li> <li>Minimal Viable Data Space Infrastructure (MVDS-IaaS)</li> </ul>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/","title":"AWS deployment","text":"<p>Warning</p> <p>Experimental guide. Errors may appear.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#aws-account-preparation","title":"AWS Account Preparation","text":"<p>Here are the steps for preparing an AWS account to deploy a Red Hat OpenShift Service (ROSA) cluster.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#required-aws-service-fees","title":"Required AWS Service Fees","text":"<p>The following table describes the AWS quotas and service levels required to create and run a Red Hat OpenShift Service (ROSA) cluster.</p> <p>Large quota requests are sent to Amazon Support for review and may take some time to be approved.</p> Service code Quota name Quota code AWS default Minimum required Description ec2 Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances L-1216C47A 5 100 Maximum number of vCPUs assigned to the Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances.The default value of 5 vCPUs is not sufficient to create ROSA clusters. ROSA has a minimum requirement of 100 vCPUs for cluster creation. ebs Storage for General Purpose SSD (gp2) volume storage in TiB L-D18FCD1D 50 300 The maximum aggregated amount of storage, in TiB, that can be provisioned across General Purpose SSD (gp2) volumes in this Region. ebs Storage for General Purpose SSD (gp3) volume storage in TiB L-7A658B76 50 300 The maximum aggregated amount of storage, in TiB, that can be provisioned across General Purpose SSD (gp3) volumes in this Region.300 TiB of storage is the required minimum for optimal performance. ebs Storage for Provisioned IOPS SSD (io1) volumes in TiB L-FD252861 50 300 The maximum aggregated amount of storage, in TiB, that can be provisioned across Provisioned IOPS SSD (io1) volumes in this Region.300 TiB of storage is the required minimum for optimal performance."},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#creating-an-iam-user","title":"Creating an IAM user","text":"<p>Creating an IAM (Identity and Access Management) user in AWS is necessary to securely manage access and permissions associated with AWS resources that will be used by the OpenShift cluster.</p> <p>We will create this user from the AWS portal IAM service (IAM &gt; Users &gt; Create User). We will then specify a user name and select the option \"Provide user access to the AWS Management Console\" and \"I want to create an IAM user\" as shown in the following image:</p> <p>After clicking on \"Next\", we will have to configure the user's permissions. We must select the option \"Attach policies directly\" and select the AdministratorAccess permissions policy.</p> <p>After creating the user, we will be shown the login details. We will be able to log in as an IAM user via the URL provided.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#aws-cloudshell-installing-the-cli","title":"AWS CloudShell: Installing the CLI","text":"<p>Once logged in with the new IAM user, the CloudShell console can be accessed from the top panel of the AWS portal.https://062041447189.signin.aws.amazon.com/console.</p> <p>Download ROSA CLI, unzip it and add it to the PATH: <pre><code>wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/rosa/latest/rosa-linux.tar.gz\ntar zxvf rosa-linux.tar.gz\nexport PATH=~:$PATH\n</code></pre></p> <p>Download Openshift CLI through ROSA: <pre><code>rosa download oc\ntar zxvf openshift-client-linux.tar.gz\n</code></pre></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#creating-the-cluster","title":"Creating the cluster","text":"<p>We will explain the steps involved in creating a Red Hat OpenShift Service cluster on AWS (ROSA).</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#login-and-quota-verification","title":"Login and quota verification","text":"<p>To log in to ROSA CLI, we need to obtain our Red Hat account token from the following link</p> <pre><code>rosa login --token=&lt;RED_HAT_TOKEN&gt;\n</code></pre> <p>Then we can check our user: <pre><code>rosa whoami\n</code></pre></p> <p></p> <p>Before proceeding with the creation of the Openshift cluster, let's check that our AWS account meets the minimum required quotas:</p> <pre><code>rosa verify quota\n</code></pre> <p></p> <p>If everything is correct, we can proceed with the creation of roles for our Red Hat account account (first time only):</p> <pre><code>rosa create account-roles --mode auto --yes\n</code></pre> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#creating-the-cluster_1","title":"Creating the cluster","text":"<p>Next, we can proceed to create the cluster. The creation time is approximately 30/40 minutes.</p> <pre><code>rosa create cluster --cluster-name rosa-dataspaces --sts --mode auto --yes\n--version 4.12.41\n</code></pre> <p>After the installation is complete, obtain the cluster information:</p> <pre><code>rosa list clusters\n</code></pre> <pre><code>rosa describe cluster --cluster &lt;name&gt;\n</code></pre> <p></p> <p>Create the cluster admin user:</p> <pre><code>rosa create admin --cluster=&lt;name&gt;\n</code></pre> <p>The user will not be available immediately, we will have to wait several minutes (about 5 minutes) before proceeding with the login to the Openshift cluster. </p> <p>Source: https://youtu.be/amLN6-JxygU?feature=shared</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#creating-hosted-zone-on-aws-route53","title":"Creating Hosted Zone on AWS Route53","text":"<p>Hosted zones on Amazon Route 53 are log containers that contain information about how you want to route traffic for a given domain and its corresponding subdomains.</p> <p>We are going to create a Hosted Zone in Route53 to route the traffic of the subdomains where we are going to host the applications of the Openshift cluster, which are the different components of the Data Space Connector.</p> <p>To do this, we need to have in our possession a given domain. In our case, the domain used is ds.smartcity-marketplace.com.</p> <p>Through the AWS services panel, we will be able to go to AWS Route53. There we can create a hosted zone by clicking on Hosted Zones &gt; Create a hosted zone. We will have to set our domain as the zone name and select \"Public hosted zone\". After creating it, we will be able to access the created zone and consult the assigned nameservers. We will have to set these name servers as the Custom DNS of our domain at the provider where we purchased it.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#domains-routes-and-certificates","title":"Domains, Routes and Certificates","text":"<p>The deployment of the FIWARE Data Space Connector assumes that you have an Openshift cluster that is able to manage domains, SSL certificates and configure routes. To achieve this, we need to install the following operators in our cluster:</p> <ul> <li> <p>External-DNS, which facilitates the automatic allocation and update of DNS records for services exposed in the cluster. When a service is created in Openshift, External-DNS detects these services and automatically creates or updates external DNS records (in our case, the hosted zone created in Route53) to point to the IP addresses of those services.</p> </li> <li> <p>Cert-Manager, which automates the issuance, renewal and management of SSL certificates in the cluster. Cert-Manager can integrate with services such as Let's Encrypt to automatically provide SSL certificates to applications that need them.</p> </li> <li> <p>Cert-Utils, which is an OpenShift-specific operator and is used to automatically inject certificates into Route objects. In OpenShift, routes are objects that define how an application is exposed to external traffic. Cert-Utils is used to automate the injection of SSL certificates into these route objects.</p> </li> </ul> <p>The process to follow to install these components is explained in the FIWARE repository: ROUTES.md.</p> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#using-external-dns","title":"Using External-DNS","text":"<p>After installing, we can check that the operator has been deployed correctly by executing:</p> <pre><code>kubectl get all -n external-dns-operator\n</code></pre> <p></p> <p>If everything is OK, we can move on to the creation of the External DNS resource that will be responsible for creating entries in our Route53 Hosted Zone.</p> <p>First we create a Kubernetes secret with the access key and the AWS secret. If we do not have this access key + secret, we can create them from the AWS IAM Portal. It is important to verify that the user associated with the access key has permissions for CRUD in Route53 (users with AdministratorAccess have these permissions).</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: aws-access-key\nnamespace: external-dns-operator\ntype: Opaque\ndata:\naws_access_key_id: &lt;BASE 64 ACCES KEY ID&gt;\naws_secret_access_key: &lt;BASE 64 ACCESS KEY SECRET&gt;\n</code></pre> <p>Note: to execute any resource in Openshift, we create a YAML file (.yaml extension) with its definition. Then run <code>kubectl apply -f &lt;YAML FILE&gt;</code>.</p> <p>The values of the secrets must be in base 64. We can use the following command to get our access ID and key in base 64:</p> <pre><code>echo -n \"&lt;BASE 64 ACCESS KEY ID&gt;\" | base64\necho -n \"&lt;BASE 64 ACCESS KEY SECRET&gt;\" | base64\n</code></pre> <p>Next, we are going to create a resource of type ExternalDNS and associate it to our Route 53 DNS Zone. The idea is that, every time a service (with assigned host) is created in the cluster, the External DNS will automatically create the entry in our DNS Zone.</p> <pre><code>apiVersion: externaldns.olm.openshift.io/v1alpha1\nkind: ExternalDNS\nmetadata:\nname: aws-routes-libelium-dev\nspec:\nprovider:\ntype: AWS\naws:\ncredentials:\nname: aws-access-key\nzones:\n- \"Z08275492EDOR2ZSK994S\"\nsource:\ntype: OpenShiftRoute\nopenshiftRouteOptions:\nrouterName: default\n</code></pre> <p>The value Z08275492EDOR2ZSK994S is the ID of the Route53 hosted zone, which in our case is the zone of the domain ds.smartcity-marketplace.com.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#using-cert-manager","title":"Using Cert Manager","text":"<p>After the installation, we can check that the operator has been deployed correctly by running by executing:</p> <pre><code>kubectl get all -n openshift-operators\n</code></pre> <p></p> <p>Now we are going to create a Kubernetes secret with the access key and the AWS secret, in the same way as we did in the previous section. This is necessary because the secrets are created at namespace level, if we want to have a secret in several namespaces, we will have to create it in each of them independently.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: aws-access-key\nnamespace: openshift-operators\ntype: Opaque\ndata:\naws_secret_access_key: &lt;BASE 64 ACCESS KEY SECRET&gt;\n</code></pre> <p>We can now move on to create a new Kubernete resource called Cluster Issuer. This resource represents Certificate Authorities (CA) that can generate signed certificates in response to requests. response to requests. In our case, we are going to create two different Cluster Issuers, one that will use Let's Encrypt as CA and another that will generate self-signed certificates. The first one is the one that we are interested in using when we do the final deployment of the connector, since it provides us with reliable certificates. provides us with trusted certificates. The second one will be used to test the deployment of the connector. connector.</p> <p>This is because during the deployment of the connector it is very likely that we will encounter errors that we will have to debug, so we will perform the deployment several times until all the components work correctly. Let's Encrypt has a policy of not generating more than 5 certificates for the same domain in a given period of time. It is crucial to take this limitation into account during the deployment process, since exceeding this limit could lead to restrictions in the issuance of certificates for our domain.</p> <p>Therefore, it is advisable to use self-signed certificates until all the connector components are working properly.</p> <p>ClusterIssuer to generate TLS certificates through Let's Encrypt: <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\nname: letsencrypt-prod\nspec:\nacme:\nemail: mj.bernal@libelium.com\npreferredChain: \"\"\nprivateKeySecretRef:\nname: lets-encrypt-prod-issuer-account-key\nserver: https://acme-v02.api.letsencrypt.org/directory\nsolvers:\n- selector:\ndnsZones:\n- \"ds.smartcity-marketplace.com\"\ndns01:\nroute53:\nregion: eu-west-1\nhostedZoneID: Z08275492EDOR2ZSK994S\naccessKeyID: AKIAQ44PVLMK76NPBH63\nsecretAccessKeySecretRef:\nname: aws-access-key\nkey: aws_secret_access_key\n</code></pre></p> <p>ClusterIssuer to generate self-signed certificates: <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\nname: self-signed\nspec:\nselfSigned: {}\n</code></pre></p> <p></p> <p>Once we have created the Cluster Issuers, let's check if they are able to request and store the certificates. store the certificates. To do this we create Certificate type resources.</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: letsencrypt-cert\nspec:\nsecretName: letsencrypt-tls-secret\nissuerRef:\nkind: ClusterIssuer\nname: letsencrypt-prod\ncommonName: \"*.ds.smartcity-marketplace.com\"\ndnsNames:\n- \"*.ds.smartcity-marketplace.com\"\n</code></pre> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: selfsigned-cert\nspec:\nsecretName: selfsigned-tls-secret\nissuerRef:\nkind: ClusterIssuer\nname: self-signed\ncommonName: \"*.ds.smartcity-marketplace.com\"\ndnsNames:\n- \"*.ds.smartcity-marketplace.com\"\n</code></pre> <p>We can check the status of the certificates by executing the following command:</p> <pre><code>kubectl get certificates\n</code></pre> <p></p> <p>If the value TRUE appears in the READY column, it means that the certificates have been successfully obtained and stored in the indicated secrets. We can also check the secrets by executing the following command:</p> <pre><code>kubectl get secrets --field-selector type=kubernetes.io/tls\n</code></pre> <p></p> <p>These secrets contain the following values:   - tls.key: private key of the certificate   - tls.crt: certificate   - ca.crt: the CA that validates the certificate.</p> <p>In the case of self-signed certificates, it only contains the first two.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#routes-and-certificates-in-fiware-ds-connector","title":"Routes and Certificates in FIWARE DS Connector","text":"<p>It is necessary to have all these Openshift components working properly in our cluster, because during the installation of the FIWARE Data Space Connector with Helm the services are exposed using these resources. The services that are exposed through a subdomain are: keyrock, activation-service, keycloak, kong, trusted-issuers-list-til, trusted-issuers-list-tir, vcverifier, vcwaltid and vcwaltid-cert.</p> <p>To expose each of these services, a number of objects are automatically created:</p> <ul> <li>Route objects: This resource is used to expose services across the network and allow external access to connector applications.      </li> <li>Certificate objects: this resource is used for the management (obtaining, maintenance...) of TLS/SSL certificates for the connector's applications.       </li> <li>Secret objects (TLS secret): these secrets contain the tls.crt and tls.key values associated with each path to services in the cluster. If these secrets are not being created correctly, the connector deployment fails. connector deployment fails.       </li> </ul>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#deployment-with-helm","title":"Deployment with Helm","text":"<p>Helm is a package manager for Kubernetes, which simplifies and automates the deployment and management of applications. A \"Helm chart\" is a pre-configured Kubernetes resource bundle, designed to be easily shared, versioned and installed using Helm.</p> <p>The FIWARE Data Space Connector is distributed as an Umbrella-Chart that contains all of the sub-charts (one for each component of the connector) and the necessary dependencies for deployment. deployment.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#download-connector-with-helm-chart","title":"Download connector with Helm Chart","text":"<p>We can download the connector via the chart repository:</p> <p><pre><code># add the repo\nhelm repo add dsc https://fiware-ops.github.io/data-space-connector/\n# Check repo\nhelm repo list\n</code></pre> Then, the chart will be ready to be used.      </p> <p>We can also download the source code (releases) directly: <pre><code># set version you are interested in\nversion=0.9.0\nwget\nhttps://github.com/FIWARE-Ops/data-space-connector/releases/download/data-sp\nace-connector-$version/data-space-connector-$version.tgz &amp;&amp; tar -xzvf\ndata-space-connector-$version.tgz\n</code></pre></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#configuration-using-valuesyaml","title":"Configuration using values.yaml","text":"<p>As we have seen, a Helm Chart is a package that contains all the necessary resources to deploy an application in Kubernetes, in our case, to launch each of the components of the connector.</p> <p>In the source code we can see that each of the components is presented as a sub-chart, which in turn has its own templates, dependencies, sub-charts and configuration files. configuration files.</p> <p></p> <p>To finish defining everything necessary for the deployment of the connector, we will have to create a global configuration file (values.yaml) in which, for each of the components, we define the necessary parameters so that the connector can be deployed in our environment. The values that we define in this global configuration file, somehow complete or overwrite the configuration parameters that come by default in the helm chart. Let's start from the configuration file provided by FIWARE: values.yaml.</p> <p>For example, the configuration for the VCWaltid component would be as follows:</p> <pre><code># Nombre del componente a configurar\nvcwaltid:\n# Permitir el despliegue del compoente vcwaltid\ndeploymentEnabled: true\n# Definir el Decentralized ID de la organizaci\u00f3n\ndid: did:web:pdc-demo.ds.smartcity-marketplace.com:did\n# Definir la ruta al host de la aplicaci\u00f3n vcwaltid\nroute:\nenabled: true\n# Subdominio que alojar\u00e1 la aplcaci\u00f3n\nhost: pdc-demo.ds.smartcity-marketplace.com\n# Configuraci\u00f3n TLS\ntls:\ninsecureEdgeTerminationPolicy: Redirect\ntermination: edge\n# Indicamos quien maneja los certificados\n# En nuestro caso el Cluster Issuer que hemos creado en pasos anteriores\ncertificate:\nissuer:\nkind: ClusterIssuer\nname: letsencrypt-prod\n# Walt-id configuraci\u00f3n interna\nvcwaltid:\n# Persistence\npersistence:\nenabled: true\npvc:\nsize: 1Gi\n# Lista de plantillas a crear\ntemplates:\nGaiaXParticipantCredential.json: |\n{\n\"@context\": [\n\"https://www.w3.org/2018/credentials/v1\",\n\"https://registry.lab.dsba.eu/development/api/trusted-shape-registry/v1/shap\nes/jsonld/trustframework#\"\n],\n\"type\": [\n\"VerifiableCredential\"\n],\n\"id\": \"did:web:raw.githubusercontent.com:egavard:payload-sign:master\",\n\"issuer\":\n\"did:web:raw.githubusercontent.com:egavard:payload-sign:master\",\n\"issuanceDate\": \"2023-03-21T12:00:00.148Z\",\n\"credentialSubject\": {\n\"id\": \"did:web:raw.githubusercontent.com:egavard:payload-sign:master\",\n\"type\": \"gx:LegalParticipant\",\n\"gx:legalName\": \"dsba compliant participant\",\n\"gx:legalRegistrationNumber\": {\n\"gx:vatID\": \"MYVATID\"\n},\n\"gx:headquarterAddress\": {\n\"gx:countrySubdivisionCode\": \"BE-BRU\"\n},\n\"gx:legalAddress\": {\n\"gx:countrySubdivisionCode\": \"BE-BRU\"\n},\n\"gx-terms-and-conditions:gaiaxTermsAndConditions\":\n\"70c1d713215f95191a11d38fe2341faed27d19e083917bc8732ca4fea4976700\"\n}\n}\nNaturalPersonCredential.json: |\n{\n\"@context\": [\"https://www.w3.org/2018/credentials/v1\"],\n\"credentialSchema\": {\n\"id\":\n\"https://raw.githubusercontent.com/FIWARE-Ops/tech-x-challenge/main/schema.j\nson\",\n\"type\": \"FullJsonSchemaValidator2021\"\n},\n\"credentialSubject\": {\n\"type\": \"gx:NaturalParticipant\",\n\"familyName\": \"Happy\",\n\"firstName\": \"User\",\n\"roles\": [{\n\"names\": [\"LEGAL_REPRESENTATIVE\"],\n\"target\": \"did:web:onboarding\"\n}]\n},\n\"id\": \"urn:uuid:3add94f4-28ec-42a1-8704-4e4aa51006b4\",\n\"issued\": \"2021-08-31T00:00:00Z\",\n\"issuer\": \"did:ebsi:2A9BZ9SUe6BatacSpvs1V5CdjHvLpQ7bEsi2Jb6LdHKnQxaN\",\n\"validFrom\": \"2021-08-31T00:00:00Z\",\n\"issuanceDate\": \"2021-08-31T00:00:00Z\",\n\"type\": [\"VerifiableCredential\", \"LegalPersonCredential\"]\n}\nMarketplaceUserCredential.json: |\n{\n\"@context\": [\"https://www.w3.org/2018/credentials/v1\"],\n\"credentialSchema\": {\n\"id\":\n\"https://raw.githubusercontent.com/FIWARE-Ops/tech-x-challenge/main/schema.j\nson\",\n\"type\": \"FullJsonSchemaValidator2021\"\n},\n\"credentialSubject\": {\n\"type\": \"gx:NaturalParticipant\",\n\"email\": \"normal-user@fiware.org\",\n\"familyName\": \"IPS\",\n\"firstName\": \"employee\",\n\"lastName\": \"IPS\",\n\"roles\": [{\n\"names\": [\"LEGAL_REPRESENTATIVE\"],\n\"target\": \"did:web:onboarding\"\n}]\n},\n\"id\": \"urn:uuid:3add94f4-28ec-42a1-8704-4e4aa51006b4\",\n\"issued\": \"2021-08-31T00:00:00Z\",\n\"issuer\": \"did:ebsi:2A9BZ9SUe6BatacSpvs1V5CdjHvLpQ7bEsi2Jb6LdHKnQxaN\",\n\"validFrom\": \"2021-08-31T00:00:00Z\",\n\"issuanceDate\": \"2021-08-31T00:00:00Z\",\n\"type\": [\"MarketplaceUserCredential\"]\n}\nEmployeeCredential.json: |\n{\n\"@context\": [\"https://www.w3.org/2018/credentials/v1\"],\n\"credentialSchema\": {\n\"id\":\n\"https://raw.githubusercontent.com/FIWARE-Ops/tech-x-challenge/main/schema.j\nson\",\n\"type\": \"FullJsonSchemaValidator2021\"\n},\n\"credentialSubject\": {\n\"type\": \"gx:NaturalParticipant\",\n\"email\": \"normal-user@fiware.org\",\n\"familyName\": \"IPS\",\n\"firstName\": \"employee\",\n\"lastName\": \"IPS\",\n\"roles\": [{\n\"names\": [\"LEGAL_REPRESENTATIVE\"],\n\"target\": \"did:web:onboarding\"\n}]\n},\n\"id\": \"urn:uuid:3add94f4-28ec-42a1-8704-4e4aa51006b4\",\n\"issued\": \"2021-08-31T00:00:00Z\",\n\"issuer\": \"did:ebsi:2A9BZ9SUe6BatacSpvs1V5CdjHvLpQ7bEsi2Jb6LdHKnQxaN\",\n\"validFrom\": \"2021-08-31T00:00:00Z\",\n\"issuanceDate\": \"2021-08-31T00:00:00Z\",\n\"type\": [\"EmployeeCredential\"]\n}\n</code></pre>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#required-secrets","title":"Required secrets","text":"<p>During the connector deployment process, we are required to have a number of secrets created in our Kubernetes namespaces.</p> <p>Some of these secrets contain the credentials that we are going to assign to certain services during their creation, so we will have to create them manually before starting the deployment. These secrets are:</p> <p></p> <p>Example of a file to define a secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysql-secret\n  namespace: pdc\ntype: Opaque\ndata:\n    dbPassword: YWRtaW4=\n    mysql-password: YWRtaW4=\n    mysql-replication-password: YWRtaW4=\n    mysql-root-password: YWRtaW4=\n</code></pre> <p>As we can see, the assigned values must be in base 64. In this case, all fields have been assigned the value \"admin\" in base 64.</p> <p>Another secret referred to in the values.yaml is the pdc-vcwaltid-tls-sec secret. This secret contains the TLS certificate of the VCWaltid component and should be created automatically during deployment, as long as our Cluster Issuer is working properly and obtaining the certificates for our services.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#ds-connector-installation","title":"DS Connector installation","text":"<p>After creating the necessary secrets and setting the necessary parameters in the values.yaml file, we can move on to deploying the connector:</p> <pre><code># Install from Helm repo\nhelm install pdc dsc/data-space-connector -n &lt;Namespace&gt; -f values.yaml\n</code></pre> <pre><code># Isstall from source code \nhelm install pdc &lt;PATH_TO_RELEASE&gt;/data-space-connector -n &lt;Namespace&gt; -f values.yaml\n</code></pre> <p>We can check if the connector has been deployed correctly by running:</p> <pre><code>kubectl get all -n &lt;Namespace&gt;\n</code></pre> <p></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#access-routes","title":"Access routes","text":"Aplicaci\u00f3n URL Descripci\u00f3n Keycloack de nuestro Package Delivery Co. https://keycloak-pdc-demo.ds.smartcity-marketplace.com/realms/fiware-server/account/#/ Keycloak to obtain Verifiable Credentials. Username and password: legal-representative Wallet https://demo-wallet.fiware.dev/ To scan QRs and obtain credentials via a smartphone Onboarding Portal https://onboarding-portal.dsba.fiware.dev To register to the FIWARE Demo Data Space Marketplace https://marketplace.dsba.fiware.dev Acceder al marketplace del Demo Data Space de FIWARE <p>Details about the onboarding process in the Data Space and access to the Marketplace: https://github.com/DOME-Marketplace/dome-gitops/blob/main/doc/DEMO.md</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#kubectl-cheat-sheet","title":"Kubectl Cheat Sheet","text":"<p>Create or delete a namespace:</p> <p><pre><code># Create\nkubectl create ns &lt;Namespace&gt;\n# Delete\nkubectl delete ns &lt;Namespace&gt;\n</code></pre> Note: When the connector deployment fails, the quickest option is to delete the namespace and recreate it. Then re-create the secrets and start the deployment with helm.</p> <p>Create a resource via a file:</p> <pre><code>kubectl apply -f &lt;FILE.YAML&gt;\n</code></pre> <p>Check the resources deployed in a namespace:</p> <pre><code># Get Pods\nkubectl get pods -n &lt;Namespace&gt;\n# Get services\nkubectl get services -n &lt;Namespace&gt;\n# Get jobs \nkubectl get jobs -n &lt;Namespace&gt;\n# Get routes\nkubectl get routes -n &lt;Namespace&gt;\n# Get secrets\nkubectl get secrets -n &lt;Namespace&gt;\n# Get certificates\nkubectl get certificates -n &lt;Namespace&gt;\n# ...\n# Get all\nkubectl get all -n &lt;Namespace&gt;\n</code></pre> <p>Check resources at cluster level:</p> <pre><code># Get Cluster Issuers list\nkubectl get clusterissuer\n# Get External DNS list\nkubectl get externaldns\n</code></pre> <p>Describe a resource (to see errors when the resource is not displayed correctly):</p> <p><pre><code>kubectl describe &lt;Resource_type&gt; &lt;Resource_name&gt; -n &lt;Namespace&gt;\n\n# Example\nkubectl describe pod keyrock-pdc-0 -n pdc\n</code></pre> View logs of a pod (usually to see application errors after deployment): <pre><code>kubectl logs &lt;Pod_name&gt; -n &lt;Namespace&gt;\n</code></pre></p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#the-ssl-secret-associated-with-vcwaltid-service-is-not-found","title":"The SSL secret associated with VCWaltid service is not found","text":"<p>TLS certificates are created as Kubernetes secrets for the components seen in section Routes and certificates in FIWARE DS Connector, but if these secrets are not being created automatically, the deployment will fail.</p> <p>The indicator that this is happening will be found when doing the kubectl describe of one of these components: Keycloak, Keyrock or dsba-pdp. This is because these components have to directly access the TLS secret created for the VCWaltid service, so an error will appear indicating that the pdc-vcwaltid-tls-sec secret has not been found in the environment.</p> <p>We have encountered this error in two cases:</p> <ul> <li>When the Cluster Issuer to obtain certificates from Let's Encrypt was not working correctly. That is, when the configuration described in section Domains, routes and Certificates had not been completed correctly.</li> </ul> <p>Solution: Check that all the resources for obtaining TLS certificates are working correctly: CertManager, External DNS and CertUtils. Also check that the domain created in Route53 belongs to us and has been correctly configured in its issuing location with the name servers indicated by AWS.</p> <ul> <li>When the number of requests to Let's Encrypt for the same domain is exceeded. If we exceed 5 requests for the same domain in the same week (this would be deploying the connector 5 times in the same week).</li> </ul> <p>Solution: Rename the subdomains to have another 5 attempts. Ideally, use the ClusterIssuer self-signed until you are sure that there are no other errors in the values.yaml. Once we know it works, switch to Let's Encrypt's ClusterIsssuer (letsencrypt-prod).</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#timed-out-waiting-for-the-condition","title":"Timed out waiting for the condition","text":"<p>If the installation with Helm does not finish successfully and you get a \"Timed out waiting for the condition\" error, it is possible that this is due to the following race condition between Keyrock and MySQL.</p> <p>During Keyrock deployment, the MySQL instance (initData section in values.yaml) is accessed. This MySQL instance has to be fully deployed for Keyrock to access it, which is not always the case. Keyrock makes 5 attempts (I tried to increase this but I didn't know how) and if it fails to access MySQL, it ends with an error.</p> <p></p> <p>No particular solution to this problem has been found, other than to try deploying again from scratch. Usually, by the fifth attempt the MySQL component is ready and the Keyrock can finish successfully, but sometimes it can take a bit longer and for that reason the whole deployment fails.</p> <p></p> <p>Before learning that the reason for this failure was due to a race condition, the following change was made to the code in the initData section of Keycloack in values.yaml:</p> <pre><code>Before:\n\n    # Init data\n    initData:\n      initEnabled: true\n      backoffLimit: 1\n      image: quay.io/wi_stefan/mysql:5.7\n\nAfter:\n\n    # Init data\n    initData:\n      initEnabled: true\n      hook: post-install\n      backoffLimit: 6\n</code></pre> <p>It might seem that the key is in the backoffLimit parameter, but the first thing that was tried was to increase this parameter. Changing this alone did not fix the problem.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#gaiaxparticipantcredential-context","title":"GaiaXParticipantCredential context","text":"<p>In the default values.yaml example for the Packaging Delivery Co. deployment, there is an error when defining the context of the GaiaXParticipantCredential.json in the VCWaltid component.</p> <p>Because of this error, QRs of type GaiaXParticipantCredential could not be read with the demo-wallet.</p> <p>The URL used was: https://registry.lab.dsba.eu/development/api/trusted-shape-registry/v1/shapes/jsonld/trustframework# Changed to: https://registry.lab.geia-x.eu/development/api/trusted-shape-registry/v1/shapes/jsonld/trustframework#</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/aws_deployment/#sources","title":"Sources","text":"<ul> <li>FIWARE Data Space Connector, FIWARE</li> <li>FIWARE Data Space Connector, FIWARE Ops</li> <li>Demo-Setup DSBA-compliant Dataspace, FIWARE Ops</li> <li>Routes &amp; Certs, FIWARE Ops</li> <li>ROSA Build - 30 mins to build a Red Hat OpenShift Service on AWS Cloud by Yongkang</li> <li>Required AWS service quotas, Red Halt Openshift</li> </ul>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/local_deployment/","title":"Local deployment","text":"<p>Warning</p> <p>This guide is still a work in progress. Errors may appear.</p> <p>Repository </p> <p>FIWARE has designed its connector in a Helm package, so you need to have a Kubernetes cluster to be able to deploy it. In order to be replicable on the largest number of systems without needing to have a large Kubernetes infrastructure, it has been decided to virtualize a cluster with Kind.</p> <p>To deploy the cluster, you must have installed: Docker, Kind, Helm, Kubectl and Terraform. You can find an example of the necessary cluster in this repository.</p> <p></p> <p>With the cluster configured, the next step is to deploy the connector following the instructions provided by FIWARE.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/","title":"Minimum Viable Data Space Infrastructure as a Service (MVDS-IaaS)","text":"<p>Warning</p> <p>This guide is still a work in progress. Errors may appear.</p> <p>Repository </p> <p>The Minimum Viable Data Space Infrastructure as a Service (MVDS-IaaS) is a set of components that are necessary to deploy a Data Space. The MVDS-IaaS is based on the FIWARE ecosystem and is designed to be deployed on top of a Kubernetes cluster. The MVDS-IaaS is composed of the following components:</p> <ul> <li>Data Space Operator (Trusted Issuers Registry)</li> <li>Data Space Connector</li> </ul>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/#on-premiselocal-deployment","title":"On premise/Local deployment","text":"<p>The MVDS-IaaS can be deployed on a local/cloud/on-premise machine using Kind. The objective is provide an autonomous environment to test the components of the Data Space with the minimum cost and complexity possible.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/#environment-setup","title":"Environment setup","text":"<p>The environment is composed of a Kubernetes cluster created with Kind, with one master node and two worker nodes (for example).</p> <p></p> <p>Other components that need to be installed in the cluster:</p> <ul> <li> <p>Ingress Nginx Controller</p> </li> <li> <p>Load Balancer</p> </li> </ul> Component Version OS Ubuntu 20.04 Docker 26.0.1 Kind 0.20.0 Helm 3.14.2 Kubernetes v1.28.3 Terraform v1.8.1"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/#status","title":"Status","text":"<p>Componet Status</p> <p>\ud83d\uded1 not started.</p> <p>\ud83d\udc77 in development...</p> <p>\u2705 running</p> Component Status Services Deployed Minimal Trust Anchor \u2705 3/3 Data Space Connector \ud83d\udc77 14/15 Data Space Operator (DSBA) \ud83d\udc77 11/12 Marketplace \ud83d\uded1 -/- <p>Detailed status</p> <p>Reference</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/deployment_status/","title":"Status","text":"<p>Warning</p> <p>This guide is still a work in progress. Errors may appear.</p> <p>Detailed local deployment status.</p>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/deployment_status/#data-space-operator","title":"Data Space Operator","text":""},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/deployment_status/#minimal-trust-anchor","title":"Minimal Trust Anchor","text":"Status (3 total): 0 \ud83d\uded1 / 0 \u274c / 0 \u26a0\ufe0f / 1 \u2753 / 2 \u2705 <p>Services Status</p> <p>\ud83d\uded1 not started.</p> <p>\u274c not running CRITICAL ERROR</p> <p>\u26a0\ufe0f running with doubts/ERRORS</p> <p>\u2753 running with doubts</p> <p>\u2705 running</p> Component Status MongoDB \u2705 Orion LD \u2705 Trusted PARTICIPANTS Registry \u2753 MongoDBOrion LD\u2753 Trusted PARTICIPANTS Registry <p>Same parameters as DSBA-compliant demo</p> Status Depends on Values.yaml Endpoint \u2705 - mongodb.yaml LoadBalancer Variables Value <code>service_name</code> <code>mongodb</code> <code>root_password</code> <code>admin</code> <p>Same parameters as DSBA-compliant demo</p> Status Depends on Values.yaml Endpoint \u2705 MongoDB orionld.yaml - Variables Value <code>service_name</code> <code>orionld</code> <code>root_password</code> <code>admin</code> <code>orion_db_name</code> <code>orion-oper</code> <p>Same parameters as DSBA-compliant demo</p> <p>Also called: Trusted Issuers Registry for FIWARE</p> Status Depends on Values.yaml Endpoint \u2753 Orion LD trusted_participants_registry.yaml <code>tpr.ds-operator.io</code>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/deployment_status/#dsba-compliant-demo","title":"DSBA-compliant demo","text":"Status (12 total): 0 \ud83d\uded1 / 1 \u274c / 3 \u26a0\ufe0f / 2 \u2753 / 6 \u2705 <p>Services Status</p> <p>\ud83d\uded1 not started.</p> <p>\u274c not running CRITICAL ERROR</p> <p>\u26a0\ufe0f running with doubts/ERRORS</p> <p>\u2753 running with doubts</p> <p>\u2705 running</p> Component Status MongoDB \u2705 MySQL \u2705 WaltID \u26a0\ufe0f Orion LD \u2705 Credentials Config Service \u2705 Trusted ISSUERS List \u2705 Trusted PARTICIPANTS Registry \u2753 Verifier \u26a0\ufe0f PDP \u2753 Kong (Proxy) \u26a0\ufe0f Portal \u2705 Keyrock \u274c MongoDBMySQL\u26a0\ufe0f WaltIDOrion LDCredentials Config ServiceTrusted ISSUERS List\u2753 Trusted PARTICIPANTS Registry\u26a0\ufe0f Verifier\u2753 PDPKong (Proxy)Portal\u274c Keyrock Status Depends on Values.yaml Endpoint \u2705 - mongodb.yaml LoadBalancer <ul> <li>values.yaml: modules/ds_operator/config/helm_values/mongodb.yaml</li> </ul> Variables Value <code>service_name</code> <code>mongodb</code> <code>root_password</code> <code>admin</code> <ul> <li>FIWARE definitions:<ul> <li><code>service name = dsba-onboarding-portal-mongodb</code> (value we assume when reading the orion ld configuration)</li> </ul> </li> </ul> Status Depends on Values.yaml Endpoint \u2705 - mysql.yaml - <ul> <li>values.yaml: modules/ds_operator/config/helm_values/mysql.yaml</li> </ul> Variables Value <code>service_name</code> <code>mysql</code> <code>root_password</code> <code>admin</code> <code>til_db</code> <code>til</code> <code>ccs_db</code> <code>ccs</code> <ul> <li>FIWARE definitions:<ul> <li><code>fullnameOverride (service name) = mysql-onboarding</code> </li> </ul> </li> </ul> Status Depends on Values.yaml Endpoint \u26a0\ufe0f - waltid.yaml <code>waltid.ds-operator.io</code> (NOT WORKING) <p>Doubts/Errors</p> <p>Ingress is configured as in other services but the corresponding endpoint is not generated.</p> <p>Other doubts:</p> <ul> <li>templates:<ul> <li>What is this credential (GaiaXParticipantCredential.json)?</li> <li>Where do you get it from? </li> <li>What is its function?</li> </ul> </li> </ul> <ul> <li>values.yaml: modules/ds_operator/config/helm_values/waltid.yaml</li> </ul> Variables Value <code>service_name</code> <code>waltid</code> <code>service_domain</code> <code>waltid.ds-operator.io</code> <code>secret_tls_name</code> <code>waltid-tls-secret</code> <code>did_domain</code> <code>did:web:waltid.ds-operator.io:did</code> <ul> <li>FIWARE definitions:<ul> <li><code>route endpoint = onboarding.dsba.fiware.dev</code></li> <li><code>did:web = did:web:onboarding.dsba.fiware.dev:did</code></li> </ul> </li> <li>endpoint type: <code>ClusterIP + Ingress</code><ul> <li>ingress: <code>waltid.ds-operator.io</code> (NOT WORKING)    </li> </ul> </li> </ul> Status Depends on Values.yaml Endpoint \u2705 MongoDB orionld.yaml - <ul> <li>values.yaml: modules/ds_operator/config/helm_values/orionld.yaml</li> </ul> Variables Value <code>service_name</code> <code>orionld</code> <code>root_password</code> <code>admin</code> <code>orion_db_name</code> <code>orion-oper</code> <ul> <li>endpoint type: <code>ClusterIP no Ingress</code></li> </ul> Status Depends on Values.yaml Endpoint \u2705 MySQL credentials_config_service.yaml - <ul> <li>values.yaml: modules/ds_operator/config/helm_values/credentials_config_service.yaml</li> </ul> Variables Value <code>service_name</code> <code>cred-conf-service</code> <code>mysql_service</code> <code>mysql</code> <code>ccs_db</code> <code>ccs</code> <code>root_password</code> <code>admin</code> <ul> <li>endpoint type: <code>ClusterIP no Ingress</code></li> </ul> Status Depends on Values.yaml Endpoint \u2705 MySQL trusted_issuers_list.yaml <code>til.ds-operator.io</code><code>tir.ds-operator.io</code> <ul> <li>values.yaml: modules/ds_operator/config/helm_values/trusted_issuers_list.yaml</li> </ul> Variables Value <code>service_name</code> <code>trusted-issuers-list</code> <code>service_domain_til</code> <code>til.ds-operator.io</code> <code>secret_tls_name_til</code> <code>trusted-issuers-list-tls-secret</code> <code>service_domain_tir</code> <code>tir.ds-operator.io</code> <code>secret_tls_name_tir</code> <code>trusted-issuers-registry-tls-secret</code> <code>mysql_service</code> <code>mysql</code> <code>root_password</code> <code>admin</code> <ul> <li>endpoint type: <code>ClusterIP + Ingress</code><ul> <li>ingress: <ul> <li>trusted issuers list: <code>til.ds-operator.io</code></li> <li>trusted issuers registry: <code>tir.ds-operator.io</code></li> </ul> </li> </ul> </li> </ul> <p>Also called: Trusted Issuers Registry for FIWARE</p> Status Depends on Values.yaml Endpoint \u2753 Orion LD trusted_participants_registry.yaml <code>tpr.ds-operator.io</code> <p>Doubts/Errors</p> <ul> <li>satellite: <ul> <li>What is this satelite? </li> <li>Where do you get it from? </li> <li>What is its function? </li> <li>Can the ID name (EU.EORI.FIWARESATELLITE) be any name or does it have to be that name for some reason?</li> </ul> </li> </ul> <ul> <li>values.yaml: modules/ds_operator/config/helm_values/trusted_participants_registry.yaml</li> </ul> Variables Value <code>service_name</code> <code>trusted-participants-registry</code> <code>service_domain</code> <code>tpr.ds-operator.io</code> <code>secret_tls_name</code> <code>trusted-participants-registry-tls-secret</code> <code>did_domain</code> <code>did:web:waltid.ds-operator.io:did</code> <code>orion_service_name</code> <code>orionld</code> <ul> <li>endpoint type: <code>ClusterIP + Ingress</code><ul> <li>ingress: <code>tpr.ds-operator.io</code></li> </ul> </li> </ul> Status Depends on Values.yaml Endpoint \u26a0\ufe0f Credentials Config ServiceTrusted ISSUERS ListWaltID verifier.yaml <code>verifier.ds-operator.io</code> <p>Doubts/Errors</p> <p>With the initContainers configuration as Fiware has it, the service is not deployed.     <pre><code>Defaulted container \"vcverifier\" out of: vcverifier, load-did (init)\n</code></pre></p> <p>Other doubts:</p> <ul> <li>m2m: <ul> <li>What is this m2m? </li> <li>Is it correctly configured in this way? </li> </ul> </li> </ul> <ul> <li>FIWARE repository:<ul> <li>code: github</li> <li>helm-chart: i4Trust v1.0.23</li> </ul> </li> <li>values.yaml: modules/ds_operator/config/helm_values/verifier.yaml</li> </ul> Variables Value <code>namespace</code> <code>ds-operator</code> <code>service_name</code> <code>verifier</code> <code>service_domain</code> <code>verifier.ds-operator.io</code> <code>secret_tls_name</code> <code>verifier-tls-secret</code> <code>waltid_service</code> <code>waltid</code> <code>tir_service</code> <code>tir.ds-operator.io</code> <code>did_domain</code> <code>did:web:waltid.ds-operator.io:did</code> <code>ccs_service</code> <code>cred-conf-service</code> <code>verifier_service</code> <code>verifier</code> <ul> <li>endpoint type: <code>ClusterIP + Ingress</code><ul> <li>ingress: <code>verifier.ds-operator.io</code></li> </ul> </li> </ul> Status Depends on Values.yaml Endpoint \u2753 WaltIDVerifierKeyrock? pdp.yaml - <p>Doubts/Errors</p> <ul> <li>ishare.existingSecret: <ul> <li>Are the secrets of waltid?</li> </ul> </li> <li>ishare.trustedFingerprints:<ul> <li>What CA is this fingerprint?</li> </ul> </li> <li>ishare.ar:<ul> <li>ar is the keyrock service?</li> </ul> </li> <li>ishare.trustAnchor:<ul> <li>What does this have to do with TPR (trusted-participants-registry)? - When talking about trustAnchor, does it refer to TPR?</li> </ul> </li> <li>additionalEnvVars:<ul> <li>What is this?</li> <li>Is it set correctly with the default value?</li> </ul> </li> </ul> <ul> <li>values.yaml: modules/ds_operator/config/helm_values/pdp.yaml</li> </ul> Variables Value <code>service_name</code> <code>pdp</code> <code>secret_tls_name_waltid</code> <code>waltid-tls-secret</code> <code>did_domain</code> <code>did:web:waltid.ds-operator.io:did</code> <code>keyrock_domain</code> <code>keyrock.ds-operator.io</code> <code>tpr_domain</code> <code>tpr.ds-operator.io</code> <code>verifier_domain</code> <code>verifier.ds-operator.io</code> <ul> <li>endpoint type: <code>ClusterIP</code></li> </ul> Status Depends on Values.yaml Endpoint \u26a0\ufe0f Orion LDPDP pdp.yaml - <p>under configuration ...</p> <pre><code>2024/03/31 11:36:52 [error] 1#0: init_by_lua error: /usr/local/share/lua/5.1/kong/init.lua:553: error parsing declarative config file /kong_dbless/kong.yml:\nin '_format_version': required field missing\nin 'apiVersion': unknown field\nin 'metadata': unknown field\nin 'data': unknown field\nin 'kind': unknown field\nstack traceback:\n    [C]: in function 'error'\n    /usr/local/share/lua/5.1/kong/init.lua:553: in function 'init'\n    init_by_lua:3: in main chunk\nnginx: [error] init_by_lua error: /usr/local/share/lua/5.1/kong/init.lua:553: error parsing declarative config file /kong_dbless/kong.yml:\nin '_format_version': required field missing\nin 'apiVersion': unknown field\nin 'metadata': unknown field\nin 'data': unknown field\nin 'kind': unknown field\nstack traceback:\n    [C]: in function 'error'\n    /usr/local/share/lua/5.1/kong/init.lua:553: in function 'init'\n    init_by_lua:3: in main chunk\n</code></pre> Status Depends on Values.yaml Endpoint \u2705 Credentials Config ServiceKongVerifier portal.yaml - Status Depends on Values.yaml Endpoint \u274c WaltIDMySQLPDP kong.yaml - <pre><code>&gt; fiware-idm@8.3.0 start /opt/fiware-idm\n&lt;!-- # &gt; node --max-http-header-size=${IDM_SERVER_MAX_HEADER_SIZE:-8192} ./bin/www\n\nConnection has been established successfully\nDatabase created\nDatabase migrated\nUnable to seed database:  Error: Command failed: npm run seed_db --silent\nERROR: Validation error\n\n    at ChildProcess.exithandler (child_process.js:383:12)\n    at ChildProcess.emit (events.js:400:28)\n    at maybeClose (internal/child_process.js:1088:16)\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:296:5) {\n  killed: false,\n  code: 1,\n  signal: null,\n  cmd: 'npm run seed_db --silent'\n}\ninternal/fs/watchers.js:251\n    throw error;\n    ^\n\nError: EMFILE: too many open files, watch '/opt/fiware-idm/etc/translations/'\n    at FSWatcher.&lt;computed&gt; (internal/fs/watchers.js:243:19)\n    at Object.watch (fs.js:1587:34)\n    at module.exports (/opt/fiware-idm/node_modules/i18n-express/index.js:68:6)\n    at Object.&lt;anonymous&gt; (/opt/fiware-idm/app.js:177:5)\n    at Module._compile (internal/modules/cjs/loader.js:1085:14)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1114:10)\n    at Module.load (internal/modules/cjs/loader.js:950:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:790:12)\n    at Module.require (internal/modules/cjs/loader.js:974:19)\n    at require (internal/modules/cjs/helpers.js:101:18)\n    at start_server (/opt/fiware-idm/bin/www:106:15)\n    at /opt/fiware-idm/bin/www:140:7\n    at /opt/fiware-idm/lib/database.js:112:11\n    at /opt/fiware-idm/lib/database.js:39:18\n    at ChildProcess.exithandler (child_process.js:390:5)\n    at ChildProcess.emit (events.js:400:28)\n    at maybeClose (internal/child_process.js:1088:16)\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:296:5) {\n  errno: -24,\n  syscall: 'watch',\n  code: 'EMFILE',\n  path: '/opt/fiware-idm/etc/translations/',\n  filename: '/opt/fiware-idm/etc/translations/'\n}\nnpm ERR! code ELIFECYCLE\nnpm ERR! errno 1\nnpm ERR! fiware-idm@8.3.0 start: `node --max-http-header-size=${IDM_SERVER_MAX_HEADER_SIZE:-8192} ./bin/www`\nnpm ERR! Exit status 1\nnpm ERR! \nnpm ERR! Failed at the fiware-idm@8.3.0 start script.\nnpm ERR! This is probably not a problem with npm. There is likely additional logging output above.\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /root/.npm/_logs/2024-03-14T13_19_08_227Z-debug.log --&gt;\n</code></pre>"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/deployment_status/#data-space-connector","title":"Data Space Connector","text":"Status (15 total): 0 \ud83d\uded1 / 1 \u274c / 0 \u26a0\ufe0f / 1 \u2753 / 13 \u2705 <p>Services Status</p> <p>\ud83d\uded1 not started.</p> <p>\u274c not running CRITICAL ERROR</p> <p>\u26a0\ufe0f running with doubts/ERRORS</p> <p>\u2753 running with doubts</p> <p>\u2705 running</p> Component Status MongoDB \u2705 MySQL \u2705 Postgres \u2705 WaltID \u2705 TM Forum API \u2705 Orion LD \u2705 Keycloak \u2705 Credentials Config Service \u2705 Trusted ISSUERS List \u2705 Verifier \u2705 Contract Management \u2705 Activation Service \u2705 Keyrock \u274c PDP \u2705 Kong (Proxy) \u2753 <p>values.yaml: modules/ds_connector/config/helm_values/connector.yaml</p> Mongo DBMySQLPostgresWaltIDTM Forum APIOrion LDCredentials Config ServiceTrusted ISSUERS ListKeycloakVerifierContract ManagementActivation Service\u274c KeyrockPDP\u2753 Kong (Proxy) Status Depends on Endpoint \u2705 - - Status Depends on Endpoint \u2705 - - Status Depends on Endpoint \u2705 - - Status Depends on Endpoint \u2705 - - Status Depends on Endpoint \u2705 - - Status Depends on Endpoint \u2705 MongoDB - Status Depends on Endpoint \u2705 MySQL - Status Depends on Endpoint \u2705 WaltIDMySQL <code>til.ds-connector.io</code><code>tir.ds-connector.io</code> Status Depends on Endpoint \u2705 WaltIDPostgres - Status Depends on Endpoint \u2705 WaltIDTrusted Participants RegistryCredentials Config Service <code>verifier.ds-connector.io</code> Status Depends on Endpoint \u2705 Trusted Issuers ListTM Forum API - Status Depends on Endpoint \u2705 MongoDB Status Depends on Endpoint \u274c MongoDB - Status Depends on Endpoint \u2705 MongoDB - Status Depends on Endpoint \u2705 MongoDB -"},{"location":"documentation/data_space_connectors/fiware/legacy_1.X.X/deployments/mvds_iaas/deployment_status/#data-space-marketplace","title":"Data Space Marketplace","text":""},{"location":"getting_started/","title":"Getting started","text":"<p>Embarking on your journey with CitCom.ai is made simple through our comprehensive \"Getting Started\" guide. This section is designed to help users familiarize themselves with the essential aspects of the CitCom.ai project. To facilitate your learning, we present a roadmap that outlines key steps and resources, ensuring you have a clear path to follow as you dive into the principles and functionalities of CitCom.ai.</p> <p>Nodes &amp; Countries</p> <p>Interoperability</p> <p>Data Spaces</p> <p>Get involved</p> <p>AI Services</p> <p>Next steps</p> <p>Ready to get started? Just click \"Next\" on the bottom navigation bar to continue!</p>"},{"location":"getting_started/get_involved/","title":"Get involved","text":"<p>CitCom.ai is a project open to the European society, allowing any interested company or institution to join this ecosystem. CitCom.ai's core technology is data spaces, which guarantee secure environments for data exchange.  </p>"},{"location":"getting_started/get_involved/#what-data-are-available","title":"What data are available?","text":"<p>There are multiple data spaces grouping different institutions (e.g. cities, companies, etc.) at local, national or European Union level. Depending on each data space, the available data may vary. </p> <ul> <li> <p> Data catalog</p> <p>Explore available data within currently deployed data spaces.</p> <p> Learn more</p> </li> </ul>"},{"location":"getting_started/get_involved/#how-can-i-join-a-data-space","title":"How can I join a data space?","text":"<p>Basically, you need to deploy a data space connector and request certified access to the data space trust anchor manager.  </p> <p>The process of joining an existing data space will be outlined, including details on membership requirements, access controls, and collaboration tools within that environment. </p> <p>In addition, access to a data space does not have to be only as a consumer of data, but you can also access it as a provider. Depending on the role you want to play in the data space, the way you join may vary. </p> <ul> <li> <p> Join a Data Space</p> <p>Understand the process of joining to an existing data space.</p> <p> Learn more</p> </li> </ul>"},{"location":"getting_started/interoperability/","title":"Interoperability","text":"<p>Interoperability is a key aspect of CitCom.ai\u2019s approach to ensuring that data can be securely and effectively shared across different systems and organizations. </p>"},{"location":"getting_started/interoperability/#idsa-documentation","title":"IDSA Documentation","text":"<ul> <li> <p>Key Layers:   The IDS documentation emphasizes four primary layers\u2014technical, semantic, organizational, and legal\u2014that collectively underpin effective interoperability.</p> </li> <li> <p>Intra Data Space Interoperability:   Within a single Data Space, a unified governance framework ensures that all participants adhere to the same protocols and models.</p> </li> <li> <p>Cross-Data Space Interoperability:   When operating across multiple Data Spaces, additional coordination is required to bridge varying protocols and legal frameworks.</p> </li> </ul> <p>For a comprehensive explanation and additional context, please refer to the original IDS documentation on Interoperability in Data Spaces.</p>"},{"location":"getting_started/interoperability/#mims","title":"MIMs","text":"<p>MIMs stands for \"Minimal Interoperability Mechanisms\". These guidelines and standards were developed by the Open &amp; Agile Smart Cities (OASC) initiative to promote interoperability among different city systems and technologies, such as traffic management systems, waste management systems, and energy distribution systems. CitCom.ai project embraces minimal interoperability mechanisms (MIMs) as part of its approach. </p>"},{"location":"getting_started/interoperability/#interoperability-levels","title":"Interoperability levels","text":"<p>Interoperability in data spaces defines how diverse systems can seamlessly exchange, interpret, and use data. Interoperability can be conceptualized as a maturity model with three levels:</p>"},{"location":"getting_started/interoperability/#level-0-custom-integration","title":"Level 0 - Custom Integration","text":"<p>At Level 0, no standard exists for data exchange. Each system is integrated via wholly customized solutions. This results in interfaces that are highly specific to each data platform. Although functional, such integration is often brittle and difficult to scale because it lacks a common vocabulary or consistent protocols. The absence of shared standards limits the potential for cross-organizational data reuse.</p>"},{"location":"getting_started/interoperability/#level-1-pivotal-interoperability-points","title":"Level 1 - Pivotal Interoperability Points","text":"<p>At Level 1, the focus shifts to identifying and adopting pivotal interoperability points among different data platforms. Key mechanisms such as MIM1 NGSI-LD and MIM2 Smart Data Models serve as the foundational standards at this stage.</p> <ul> <li> <p>MIM1 NGSI-LD provides a standardized API for context information management, enabling different systems to share and retrieve structured data consistently.</p> </li> <li> <p>MIM2 Smart Data Models standardize how entities and their attributes are defined, ensuring that data from disparate sources uses a common language.</p> </li> </ul> <p>This level establishes core interoperability through shared data models and context management, even though individual systems may retain internal heterogeneity. </p>"},{"location":"getting_started/interoperability/#level-2-common-interface-with-integrated-security","title":"Level 2 - Common Interface with Integrated Security","text":"<p>At Level 2, interoperability is further enhanced by defining a standard interface typically through deploying a DS Connector. This connector not only leverages the standards from Level 1 but also integrates a comprehensive security layer that includes:</p> <ul> <li> <p>Trust Frameworks: Mechanisms to establish and maintain trust among participants.</p> </li> <li> <p>Identity Management: Standardized approaches to manage and verify participant identities.</p> </li> <li> <p>Authorization and Trust Services: Policies and registries that enforce data usage rules and access control.</p> </li> </ul> <p>This unified interface simplifies plug-and-play integration and ensures that all data transactions are secure, standardized, and governed under a common framework</p> <p>Learn more about this</p> <p>Check Interoperability in Data Spaces section from IDSA. Also check MIMs Toolkit section or OASC MIMs 2024 for more details.</p>"},{"location":"getting_started/nodes_countries/","title":"Nodes &amp; Countries","text":"<p>CitCom is organized as three \"supernodes\" Nordic, Central, and South, with satellites and sub-nodes located across 11 countries in the European Union: Denmark, Sweden, Finland, the Netherlands, Belgium, Luxembourg, France, Germany, Spain, Poland, and Italy. </p> <ul> <li>Nordic Supernode is focused on the theme \"POWER.\" Nordic nodes work in areas that support smart cities and communities and focus on energy, environmental solutions, cyber security, ethics, and edge learning.</li> <li>Central Supernode revolves around the \"MOVE\" theme. The node focuses more specifically on challenges related to mobility and logistics in cities and communities.</li> <li>Southern Supernode revolves around the \"CONNECT\" theme. The supernode focuses more specifically on the need to securely connect citizens, infrastructures, AI, and robotics services in cities and communities. This theme and supernode will focus on innovations that provide intelligence to local infrastructures and city cross-sectoral services.</li> </ul> <p>Supernodes serve as regional hubs that aggregate expertise, resources, and best practices, thereby providing strategic oversight and ensuring that the testing environments remain closely aligned with the unique challenges and opportunities of their respective regions.</p> The Nordic Supernode The Central Supernode The Southern Supernode Focus Areas POWER MOBILITY CONNECTIVITY Lead Denmark Belgium Spain Members Denmark, Sweden, Finland Belgium, France, Luxembourg, The Netherlands Spain, Italy, Poland, Germany <p>Learn more about this</p> <p>Check the TEF nodes section for further details.</p>"},{"location":"getting_started/data_spaces/","title":"Data spaces","text":"<p>Data spaces refer to structured and managed environments where data from various sources is securely stored, shared, and utilized for AI and robotics applications within smart and sustainable cities. These data spaces are the project's core technology, enabling participants to access and leverage high-quality data for testing, experimentation, and validation of AI technologies.</p> <p>Data spaces support interoperability, ensuring that data from different sources can be combined and used while complying with regulations such as the GDPR and other EU directives. They provide the necessary infrastructure for managing data in a way that supports ethical considerations, cybersecurity, and the broader goals of creating a more digital and sustainable urban environment.</p> <p>In CitCom.ai, data spaces are pivotal in accelerating innovation by facilitating collaboration among different stakeholders. They offer a secure and compliant framework for data exchange, ensuring that the AI solutions developed within the project are both reliable and aligned with European standards.</p> <p>Basically, a data space must be formed, at least, by the following components (Minimum Data Space Reference): </p> <ul> <li> <p>Trust Anchor (TA): Responsible for managing trust in the data space. It is the manager of the identities of the different elements of the data space and of managing the trust in them. At least one TA shall exist in the data space, managed by the organization in charge of the data space. </p> </li> <li> <p>Data Space Connector (DSC): Responsible for managing the communication between the different elements of the data space. It oversees managing authentication, authorization and data access control. There must be at least two DSCs, one per organization, to be able to affirm that a data space exists.</p> </li> </ul> <p></p> <p>More details</p> <p>Overview of open-source data spaces connectors: Overview section</p>"},{"location":"getting_started/data_spaces/deployment/","title":"Data Space Deployment","text":"<p>As we have seen in the Data Space - Join one section, it is mandatory to deploy a connector in each organization that wants to share data in a Data Space. This connector is responsible for managing the data sharing process with the rest of the Data Space members.</p> <p>These are the instructions to deploy a Data Space from scratch with Fiware technologies.</p>"},{"location":"getting_started/data_spaces/deployment/#minimal-viable-data-space","title":"Minimal Viable Data Space","text":"<p>The minimal viable data space is composed of a Trust Anchor and a Data Space Connector (Reference).</p>"},{"location":"getting_started/data_spaces/deployment/#fiware-data-space-connector","title":"FIWARE Data Space Connector","text":"<p>FIWARE developed a Data Space Connector that can be deployed in a local environment. This connector is a minimal version of the Data Space Connector that can be used to test the Data Space functionalities.</p>"},{"location":"getting_started/data_spaces/deployment/#from-scratch","title":"From scratch","text":"<p>This section describes how to deploy a Data Space from scratch in different scenarios:</p> <ul> <li>Cloud provider.</li> <li>On-premises infrastructure.</li> </ul> <p>Warning</p> <p>This guide is a work in progress. It will be updated with more detailed in the next months. Terraform deployment.</p>"},{"location":"getting_started/data_spaces/deployment/#data-federation","title":"Data Federation","text":"<p>The Data Federation is a more complex scenario where multiple Data Spaces or data platform are federated to share data. Depending on the technology used, the federation process can be different.</p> <p>Reference</p>"},{"location":"getting_started/data_spaces/join/","title":"Join a Data Space","text":"<p>Info</p> <p>CitCom.ai uses data spaces based on FIWARE technology.</p> <p>The initial adoption of the FIWARE Data Space connector (DSC) within the CitCom.ai project is a strategic decision that aligns with the Data Space Business Alliance (DSBA) recommendations, ensuring a robust and interoperable framework for data exchange across Testing and Experimentation Facilities (TEFs). The FIWARE DSC is recognized for its compliance with open standards and ability to facilitate secure, efficient data sharing between diverse platforms and ecosystems.</p> <p>To access to a data space, you mainly need: </p> <ol> <li> <p>A digital certificate: To be able to identify yourself as an organization within the data space. </p> </li> <li> <p>A data space connector: To be able to communicate with the data space.  Data Space Deployment. </p> </li> </ol>"},{"location":"getting_started/data_spaces/join/#sign-up","title":"Sign Up","text":"<p>Current sign up process</p> <p>In general, it will be necessary to contact via email the managers of the data space so that they authorize our organization.</p> <p>Depending on the configuration of the data space, the registration process may vary.  </p> <p>Currently, most commonly, you will need to contact the data space TA administrator for information on the type of certificate you need and how to provide it so that they can authorize you as an authorized entity in the data space. </p> <p>In the future, this process will be automated, and you will be able to do it directly from the data space platform. Using the European digital identity, you will be able to register in the data space in a simple and secure way. </p>"},{"location":"getting_started/data_spaces/join/#data-space-connector","title":"Data Space Connector","text":"<p>The Data Space Connector (DSC) is a software component that is responsible for managing the communication between the different elements of the data space. It oversees managing authentication, authorization, and data access control.</p> <p>Fiware provides a reference implementation of the DSC, which is available in the Fiware GitHub repository</p> <p>See the deployment instructions for more details.</p>"},{"location":"services/","title":"AI Services","text":"<p>Minimal interoperable AI services for platforms compatible with MIMs:</p> <ul> <li> <p> Waste Collection (MIAIS)</p> <p> Optimize city waste collection routes.</p> <p> Deployment guide</p> </li> <li> <p> Locating bins &amp; dumpsters</p> <p></p> <p>Optimize bin &amp; dumpster locations.</p> <p> Deployment guide</p> </li> </ul>"},{"location":"services/waste_collection/","title":"Minimal Interoperable AI Service (MIAIS) - Waste Collection","text":"<p>Repository </p> <p></p>"},{"location":"services/waste_collection/#introduction","title":"Introduction","text":"<p>This guide shows how to deploy an AI-based service to optimize city waste collection using context information and the Openroute optimization service. As a limited example and possible starting point to build your own, it illustrates a Minimal Interoperable AI Service (MIAIS) that follows MIMs embraced by the CitCom.ai project.</p> <p>Scenario</p> <p>Different sensors are deployed throughout the city to monitor the fill levels of waste containers. These sensors periodically collect data on the fill levels and send it to the TEF site data platform. The goal is to use this context information to create optimal truck waste collection routes. The solution will only consider the current waste container filling level, their location, available trucks, and start and end location.</p> <p>Do not worry if your TEF site lacks some components from the described scenario. As a demo, we offer a docker instance with all the necessary components to quickly test the service, including some dummy data. Further sections explain how to adapt and deploy the service on your TEF site. To successfully deploy the service in a real environment, you must meet the minimum requirements below or at least be close to them. </p>"},{"location":"services/waste_collection/#minimal-requirements","title":"Minimal requirements","text":"<p>Below are the minimum requirements for deploying the service on your TEF site. Please remember that you can always test the service via the demo if you still need to meet them.</p>"},{"location":"services/waste_collection/#ngsi-ld-for-context-information-mim1","title":"NGSI-LD for Context Information (MIM1)","text":"<pre><code>graph LR\n  A[\"\ud83c\udfdb\ufe0f TEF site data platform\"] ---|NGSI-LD| B[\"\ud83e\udd16 AI service\"];\n  B --- C[\"\ud83d\udcc8 Dashboard\"];\n  C[\"\ud83d\udcc8 Dashboard\"] --- D[\"\ud83d\udc65 Users\"];</code></pre> <p>The above image shows the overall architecture: The AI service gets the necessary information from the TEF site data platform using the NGSI-LD specification compliant with MIM1. In the future, once the data space connector is deployed, the AI service will get the data through it. </p> <p>An intermediary adapter may be required in cases where the city data platform does not comply with the proposed NGSI-LD standard. If your current data platform uses the NGSIv2 specification, check the documentation section for more details about using Lepus or connecting an NGSI-V2 broker with an NGSI-LD broker through subscriptions. </p> <p>The AI service will use the gathered information to offer an interactive service through a web dashboard. Once the user provides a desired config the AI service will produce an optimal solution.</p>"},{"location":"services/waste_collection/#smartdatamodels-for-entities-mim2","title":"SmartDataModels for entities (MIM2)","text":"<p>The following entities are retrieved from the TEF site data platform and used in the service: WasteContainer and Vehicle. Feel free to click on them and explore their corresponding Smart Data Model specifications.</p>"},{"location":"services/waste_collection/#openroute-api-key","title":"Openroute API key","text":"<p>Openroute offers a free vehicle routing optimization service based on the Vroom project. The MIAIS uses this service to provide and optimal solution. To access the service you will need a valid API key, so go over to openrouteservice.org and get one; you will need it later. If you want to learn more, the API and parameters specification are explained on the Vroom repository.</p>"},{"location":"services/waste_collection/#deploying-the-demo","title":"Deploying the demo","text":"<p>As mentioned, a demo example with some dummy data has been provided so partners can quickly test the service without worrying about the minimal requirements. Below, you will find step-by-step instructions on deploying the minimal interoperable service for waste collection using docker. </p> <ol> <li> <p>Clone the repository and navigate to its root folder: <pre><code>git clone https://github.com/CitCom-VRAIN/waste-collection-demo.git &amp;&amp; cd waste-collection-demo\n</code></pre></p> </li> <li> <p>Init git submodules with the following command. This will clone and install a dead simple ngsi-ld client library in <code>lib</code> folder. Please note that the library is for testing purposes only and lacks most functionality. However, it quickly allows you to implement your own methods to interact with the context broker. <pre><code>git submodule init &amp;&amp; git submodule update\n</code></pre></p> </li> <li> <p>Next, create and run the Orion-LD Docker image. It is necessary to have Docker and Docker Compose installed. This will set-up an Orion-LD broker with a MongoDB database. Check out the <code>docker-compose.yaml</code> file for more details. Alternatively, the demo can also be deployed by using a Scorpio NGSI-LD Broker. If you feel more familiar with this case, switch the branch to <code>mvs-scorpiold</code> before running <code>docker compose up</code>: <pre><code># Optional: Switch to Scorpio NGSI-LD Broker\n# git checkout mvs-scorpiold\ndocker compose up\n</code></pre></p> </li> <li> <p>Create and activate a Python virtual environment: <pre><code>python3 -m venv ./venv &amp;&amp; source ./venv/bin/activate\n</code></pre></p> </li> <li> <p>Install all requirements: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Create an <code>.env</code> file using <code>.env.example</code> as a guide:  <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Then edit the <code>.env</code> file and replace the <code>OPENROUTESERVICE_API_KEY</code> value with your own Openroute service API key. <pre><code>OPENROUTESERVICE_API_KEY=\"Replace this string with your Openroute API key\"\n</code></pre></p> </li> <li> <p>After editing the file and saving it, read the .env file: <pre><code>source .env\n</code></pre></p> </li> <li> <p>Populate the context broker with some fake data by running the following command. This will create some <code>WasteContainer</code> and <code>VehicleModel</code> entities in the broker: <pre><code>python3 upsert_fake_data.py\n</code></pre></p> </li> <li> <p>Finally, start the server and open http://127.0.0.1:5000 in your browser: <pre><code>flask --app server run\n</code></pre></p> </li> </ol>"},{"location":"services/waste_collection/#deploying-in-your-tef","title":"Deploying in your TEF","text":"<p>If your TEF site meets all minimum requirements, you can go over deploying the MIAIS in your city. Start by changing your <code>.env</code> variables so they point to your real data platform. However, some changes, such as implementing an authentication method, may be required. The Minimal Interoperable AI Service is a starting point; therefore, feel free to explore and edit the project to start building it up on your own. Here are some tips that can help you adapt this example to your needs:</p> Project structure <ul> <li><code>static/</code>: Frontend folder. <ul> <li><code>index.html</code>: Defines the UI</li> <li><code>main.js</code>: Defines the main logic.</li> <li><code>modules/</code>: Includes entity classes, API rest client, optimization logic, leaflet stuff, and UI functions.</li> <li><code>style.css</code>: Main CSS style sheet.</li> </ul> </li> <li><code>server.py</code>:  Sets up the Flask server and exposes the service API.</li> <li><code>services/Optimization.py</code>: Defines the query for Openroute optimization service. </li> <li><code>lib/</code>: External python libraries.</li> </ul> Formulate new delivery/pickup problems <p>We worked with optimizing trucks routes (<code>Vehicle</code> entity) to pickup <code>WasteContainers</code> that are full (<code>fillingLevel</code> attribute). However, these entities can be replaced to formulate new delivery/pickup problems. Visit the Smart Data Models repositories for more entities and decide which attributes are relevant for your problem. To integrate these changes in the project, you should create the corresponding entity classes in <code>static/modules/</code> folder, just like <code>WasteContainer.js</code>. Take also a look at <code>main.js</code> and modify it accordingly. </p> <p>Moreover, maybe your situation needs to consider some time restrictions or priorities. Check out the Openroute service API specification, which is powerful and includes many parameters to fit your optimization needs. To change/add additional query parameters, go over <code>Optimization.py</code> and <code>Optimizer.js</code> files.</p> Level 1: Authentication <p>When working with brokers in a production state, authentication is often required. The <code>ngsild-client</code> library included in the example does not come with authentication support. However, it is quite straightforward to extend it to meet authentication requirements. As an example, see the following code from the Valencia TEF site implementation, which implementes authentication for their NGSIv2 setup.</p> <pre><code>from lib.ngsildclient.Auth import Authv2\nfrom lib.ngsildclient.Client import Client\n\n\n# Define service &amp; subservice\nservice = \"tef_city\"\nsubservice = \"/containers\"\n\n# Authenticate\nauth = Authv2()\ntoken = auth.get_auth_token_subservice(service, subservice)\n\n# Ngsi-ld broker client\nclient = Client()\n\n# Fetch WasteContainer entities\ncontext = os.environ.get(\"WASTECONTAINERS_CONTEXT\")\ncontainers = client.get_all_entities_by_type(\"WasteContainer\", context, 100, 0, service, subservice, token).json()\n</code></pre> <p>Environment variables in <code>.env</code> file:</p> <pre><code>AUTH_PROTOCOL=\"https\"\nENDPOINT_KEYSTONE=\"auth.tef.com:15000\"\nAUTH_USER=\"xxxxx\"\nAUTH_PASSWORD=\"xxxxx\"\n</code></pre> Level 2: Data Space Connector Authentication <p>If your TEF has a data space connector deployed, you can use the <code>fdsauth</code> python library to authenticate and retrieve the corresponding token. To install <code>fdsauth</code>, simply use <code>pip</code>:   <pre><code>pip install fdsauth\n</code></pre>   Next, a DID (Decentralized Identifier) and the corresponding key-material is required. You can create such via:   <pre><code>mkdir certs &amp;&amp; cd certs\ndocker run -v $(pwd):/cert quay.io/wi_stefan/did-helper:0.1.1\n</code></pre></p> <p>Then, use the following example code to obtain your authentication token:   <pre><code>from fdsauth import Consumer\nimport requests\n\nconsumer = Consumer(\n    keycloak_protocol=\"http\",\n    keycloak_endpoint=\"keycloak.consumer-a.local\",\n    keycloak_realm_path=\"realms/test-realm/protocol\",\n    keycloak_user_name=\"test-user\",\n    keycloak_user_password=\"test\",\n    apisix_protocol=\"http\",\n    apisix_endpoint=\"apisix-proxy.provider-a.local\",\n    certs_path=\"./certs\",\n)\n\ntoken = consumer.get_data_service_access_token()\n\ntry:\n    # Attempt to access data using the obtained service token. Get entities of type EnergyReport.\n    url = f\"http://apisix-proxy.provider-a.local/ngsi-ld/v1/entities?type=EnergyReport\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Authorization\": f\"Bearer {token}\",\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    print(response.json())\nexcept Exception as req_err:\n    print(f\"Request error occurred: {req_err}\")\n</code></pre>   For more details, check out the fdsauth repository.</p>"},{"location":"services/waste_collection/#track-and-status-of-known-problems","title":"Track and status of known problems","text":"<ul> <li> Openroute optimization service has a maximum limit of 70 locations. This can be solved by deploying your own Openroute instance.</li> <li> Solutions offered by the AI service should also be provided following MIM1 and MIM2 recommendations. Eg: using Smart data models format like (FleetVehicle, FleetVehicleOperation, Road and RoadSegment).</li> </ul>"},{"location":"services/wastecontainer_location_optimization/","title":"Optimizing WasteContainer Location","text":"<p>Repository </p> <p></p>"},{"location":"services/wastecontainer_location_optimization/#introduction","title":"Introduction","text":"<p>This AI services optimizes the placement of used cooking oil (UCO) containers in Valencia to improve urban recycling accessibility and promote environmental sustainability. The service dresses the maximum covering location problem by proposing a genetic algorithm that utilizes context information to strategically position UCO recycling bins throughout the city. The goal is to enhance accessibility for residents while reducing operational costs and environmental impact. </p> <pre><code>graph TD;\n    A[Web Frontend] --&gt;|Sends optimization task| B[REST API];\n    B --&gt;|Submits task to| C[Celery];\n    C --&gt;|Assigns to a worker| D[Optimization Task];\n    D --&gt;|Fetches data from| G[Redis];\n    C --&gt;|Publishes tasks status updates to| F[MQTT Broker];\n\n    C --&gt;|Assigns to a worker| H[Periodically Update Data Task ];\n    H --&gt;|Gets data from| I[City Data Platform];\n    H --&gt;|Stores data in| G[Redis];</code></pre>"},{"location":"services/wastecontainer_location_optimization/#deploy","title":"Deploy","text":"<ol> <li> <p>Clone the repository and navigate to its root folder: <pre><code>git clone https://github.com/CitCom-VRAIN/optimizing-container-location.git &amp;&amp; cd optimizing-container-location\n</code></pre></p> </li> <li> <p>Create and activate a Python virtual environment: <pre><code>python3 -m venv ./venv &amp;&amp; source ./venv/bin/activate\n</code></pre></p> </li> <li> <p>Install all requirements: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Create an <code>.env</code> file using <code>.env.example</code> as a guide:  <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Then edit the <code>env</code> variables so they point to your real data platform. <pre><code>ENDPOINT_CB=\"127.0.0.1\"\nENDPOINT_CB_PORT=\"9090\"\nENDPOINT_REDIS=\"localhost\"\nREDIS_PORT=\"6379\"\nENDPOINT_CELERY_BROKER=\"pyamqp://guest@localhost//\"\n</code></pre></p> </li> <li> <p>Run Redis Container <pre><code>docker run --name ocl-redis -d -p 6379:6379 redis\n</code></pre></p> </li> <li> <p>Run RabbitMQ Container <pre><code>docker run --name ocl-rabbitmq -d -p 5672:5672 rabbitmq\n</code></pre></p> </li> <li> <p>Start Celery Worker <pre><code>celery -A tasks worker --loglevel=info\n</code></pre></p> </li> <li> <p>Finally, start the server and open http://127.0.0.1:5000 in your browser: <pre><code>flask --app server run\n</code></pre></p> </li> </ol>"},{"location":"tef/","title":"TEF Nodes","text":"<p>Overview of the current Testing and Experimentation Facility (TEF) nodes / sites.</p> Node/Site Focus/Theme Infrastructure Nordic Supernode POWER Living labs, and research facilities DOLL Living Lab Energy-efficient solutions Physical testbed for energy-efficient lighting Aarhus City Lab Smart City solutions Urban living lab, technical infrastructure DTU (Denmark) Technical and natural sciences Labs for research, innovation, entrepreneurship Center Denmark Cross-sectoral data High quality cross-sectoral data platform GATE21 Sustainable and healthy future Mobile technical testing facility Tampere (Finland) Smart city and AI-powered solutions Physical and digital testbeds RISE (Sweden) Research and innovation partner Testbeds, research facilities Central Supernode MOVE Urban, mobility Mechelen (Belgium) Urban mobility Urban mobility experiments, sensor installation Brussels (Belgium) Urban combined mobility Urban mobility tracking and analytics Eindhoven (Netherlands) Sustainable mobility Smart intersection, sensor technology Paris (France) Autonomous vehicles Urban driving tracks, simulation testing Luxembourg (Luxembourg) Electromobility Prototyping, testing in electromobility Southern Supernode CONNECT Urban, AI, robotics Valencia (Spain) Smart Cities, Climate Mission Living lab, sensor network Milano (Italy) Urban regeneration, facility management Urban regeneration, high-tech features Warsaw (Poland) Smart Sustainable Cities and Communities Spatial big data, ma"},{"location":"toolbox/","title":"Toolbox","text":"<p>A set of useful tools compatible with MIMs.</p> <ul> <li>AI Toolkit</li> <li>MIMs Toolkit</li> <li>Other Tools</li> </ul>"},{"location":"toolbox/ai_logging_monitor/","title":"AI system Logging &amp; Monitor","text":"<p>Repository </p>"},{"location":"toolbox/ai_logging_monitor/#how-to-use","title":"How to use","text":"<ol> <li>Download or clone repository.</li> <li>Run <code>pip install -r requeriments.txt</code> + Enter.</li> <li>Run the <code>deploy.sh</code> file, which deploys each service independently:</li> <li>The logging (external) can be consulted at WhyLabs.</li> <li>The monitor interface (internal) can be consulted on port <code>5004</code>.</li> </ol>"},{"location":"toolbox/ai_logging_monitor/#previous-considerations","title":"Previous considerations","text":"<p>For the operation of the Logging and Monitor modules, the ones exposed in this repository, it is necessary a database that stores the predictions and revisions in the expected way, this is:</p> <ul> <li><code>filename</code>: (string) base name of the image.</li> <li><code>output_confidence</code>: (float) (optional) confidence of the max class predicted.</li> <li><code>output_prediction</code>: (int) predicted class.</li> <li><code>output_validated</code>: (bool) if it is validated the prediction.</li> <li><code>timestamp</code>: (string/timestamp) timestamp of the prediction.</li> <li><code>trained</code>: (bool) (optional) if it is already used for retraining.</li> <li><code>logged</code>: (bool) if is is already logged to the external logging.</li> <li><code>timestamp_validated</code>: (string/timestamp) timestamp of the validation.</li> </ul> <p>A MongoDB has been used in the implementation. If you wish to use another database, it would be necessary to update the corresponding part of the code, maintaining the described structure.</p>"},{"location":"toolbox/ai_logging_monitor/#logging-external","title":"Logging (External)","text":"<p>The Logging module tracks the model's performance in production. It logs various performance metrics, including accuracy, precision, recall, and the number of predictions made over time. A monitoring platform visualizes these metrics, helping identify trends and potential issues. Additionally, this module monitors the system for any anomalies or significant drops in performance, triggering alerts when necessary to ensure the model remains reliable and any problems are promptly addressed.</p>"},{"location":"toolbox/ai_logging_monitor/#key-features","title":"Key features","text":"<p>Key features of the module include performance logging, drift detection, data storage, and scheduling.</p> <ul> <li>Performance logging involves logging various aspects of the classification model's performance.</li> <li>Drift detection identifies any changes in the model's performance over time.</li> <li>Data storage ensures that logged performance metrics and related metadata are stored in a database, providing a robust system for retrieving and analyzing historical performance data.</li> <li>A scheduler ensures regular monitoring tasks, ensuring continuous and timely monitoring of the model's performance.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#functionalities","title":"Functionalities","text":"<p>The module's functionalities include environment configuration, database connection, monitoring tasks, scheduler initialization, and error handling.</p> <ul> <li>Environment configuration involves setting environment variables for the monitoring platform.</li> <li>Database connection uses the provided URI and database name to establish a connection to the database, ensuring reliable database access.</li> <li>The monitoring task fetches new entries from the database and logs their performance metrics, integrating with the logging platform to create and store performance logs.</li> <li>Scheduler initialization sets up a blocking scheduler to run the monitoring task regularly, ensuring the monitoring process runs continuously without manual intervention.</li> <li>Error handling entails logging errors encountered during database connection and monitoring tasks, and providing feedback on the status of the monitoring process.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#implementation","title":"Implementation","text":"<p>The module has been implemented using WhyLogs and WhyLabs. WhyLogs is a logging library that captures various performance metrics, and WhyLabs is a platform that helps identify trends and potential issues.</p> <p></p>"},{"location":"toolbox/ai_logging_monitor/#monitor-internal","title":"Monitor (Internal)","text":"<p>The Monitor module is designed to track the performance of the classification model in production. Using charts, it provides a web interface that visualizes various performance metrics over a selected time range. The module utilizes a web framework for the backend, a database for data storage, and a charting library for creating interactive charts.</p>"},{"location":"toolbox/ai_logging_monitor/#key-features_1","title":"Key features","text":"<p>Key features of this module include a web interface, data fetching, and charts.</p> <ul> <li>The web interface allows users to monitor model performance through various tabs.</li> <li>Data fetching retrieves logs based on user-specified date ranges, and performance metrics such as precision, accuracy, recall, and F1 score are calculated.</li> <li>Interactive charts visualize these metrics, including line charts for overall metrics and bar charts for class-specific metrics.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#functionalities_1","title":"Functionalities","text":"<p>The detailed functionalities include environment configuration, database connection, metrics calculation, chart initialization, and web interface tabs.</p> <ul> <li>Environment configuration involves setting up connection details for the database.</li> <li>A connection to the database is established using the provided URI and database name, and data is fetched based on the user-specified date range.</li> <li>Metrics calculation computes daily precision, accuracy, recall, and F1 score, which are then aggregated to provide an overall performance view.</li> <li>Chart initialization visualizes these metrics, with line charts for daily metrics and bar charts for metrics by class.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#web-interface-tabs","title":"Web interface tabs","text":"<p>The web interface includes several tabs:</p> <ul> <li>The main tab provides an overview of the validated samples and metrics.</li> <li>The metrics tab displays separate line charts for precision, accuracy, recall, and F1 score.</li> <li>The samples by class tab features a pie chart of validated samples by class.</li> <li>The metrics by class tab displays bar charts of precision, recall, and F1 score by class.</li> <li>The training data tab includes a pie chart of samples by class from the training dataset.</li> </ul>"},{"location":"toolbox/ai_logging_monitor/#implementation_1","title":"Implementation","text":"<p>The module has been implemented using Flask and Chart.js, a JavaScript library for creating interactive charts.</p> <p></p>"},{"location":"toolbox/ai_logging_monitor/#alternatives","title":"Alternatives","text":"<p>Additionally, other libraries and platforms were considered:</p> <ul> <li>Evidently, a tool for evaluating and monitoring the performance of machine learning models, providing detailed reports and visualizations of model metrics.</li> <li>Grafana + Prometheus, where Grafana is an open-source platform for monitoring and observability, providing dashboards and visualizations, and Prometheus is a monitoring system and time-series database that collects metrics and data.</li> <li>alibi-detect, a library focused on detecting outliers, adversarial instances, and concept drift in machine learning models, ensuring the robustness and reliability of the deployed models.</li> </ul>"},{"location":"toolbox/ai_toolkit/","title":"AI Toolkit","text":"<p>For the development and implementation of different AI services, here we list a series of projects that can significantly help in managing these services.</p>"},{"location":"toolbox/ai_toolkit/#machine-learning","title":"Machine Learning","text":"Framework Ray Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads. ZenML Develop ML pipelines locally that run on any MLOps stack. Prefect Modern workflow orchestration for data and ML engineers. Platform Kubeflow Machine Learning Toolkit for Kubernetes. Weights &amp; Biases Weights &amp; Biases helps AI developers build better models faster. Quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, and manage your ML workflows end-to-end. MLflow Open source platform for the machine learning lifecycle. Library SciKit-Learn Machine learning in Python XGBoost Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for Python, R, Java, Scala, C++ and more. Runs on a single machine, Hadoop, Spark, Dask, Flink and DataFlow. Darts A python library for user-friendly forecasting and anomaly detection on time series. OpenCV Open Source Computer Vision Library."},{"location":"toolbox/ai_toolkit/#model","title":"Model","text":"Format &amp; Interface ONNX Open standard for machine learning interoperability. Workflow Airflow A platform to programmatically author, schedule, and monitor workflows. Nifi NiFi automates cybersecurity, observability, event streams, and generative AI data pipelines and distribution for thousands of companies worldwide across every industry."},{"location":"toolbox/ai_toolkit/#deep-learning","title":"Deep Learning","text":"Framework Tensorflow Pytorch Tensors and Dynamic neural networks in Python with strong GPU acceleration. Library Keras Deep Learning for humans. Pytorch Lightning Deep learning framework to train, deploy, and ship AI products Lightning fast. RAPIDS RAPIDS provides unmatched speed with familiar APIs that match the most popular PyData libraries. Built on state-of-the-art foundations like NVIDIA CUDA and Apache Arrow, it unlocks the speed of GPUs with code you already know. OpenMMLab Covers a wide range of research topics of computer vision, e.g., classification, detection, segmentation and super-resolution."},{"location":"toolbox/ai_toolkit/#programming","title":"Programming","text":"Language Python The Python programming language. Library Dask Parallel computing with task scheduling. Numpy The fundamental package for scientific computing with Python. Hydra Hydra is a framework for elegantly configuring complex applications SciPy SciPy library main repository."},{"location":"toolbox/ai_toolkit/#notebook-environment","title":"Notebook Environment","text":"Notebook Environment Jupyter Jupyter Interactive Notebook. Colab Python libraries for Google Colaboratory."},{"location":"toolbox/ai_toolkit/#distributed-computing","title":"Distributed Computing","text":"Computing &amp; Management Docker Podman A tool for managing OCI containers and pods. Kubernetes An open-source system for automating deployment, scaling, and management of containerized applications. Spark A unified analytics engine for large-scale data processing. Portainer Portainer is the most versatile container management software that simplifies your secure adoption of containers with remarkable speed. OpenShift Unified platform to build, modernize, and deploy applications at scale. Work smarter and faster with a complete set of services for bringing apps to market on your choice of infrastructure. ArgoCD Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes."},{"location":"toolbox/ai_toolkit/#data","title":"Data","text":"Relation DB MySQL MySQL Server, the world's most popular open source database, and MySQL Cluster, a real-time, open source transactional database. Postgres Develop ML pipelines locally that run on any MLOps stack. Storage &amp; Format Delta Lake An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs. influxdb Scalable datastore for metrics, events, and real-time analytics. pandas Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more. Versioning DVC ML Experiments Management with Git. Operations Whylogs An open-source data logging library for machine learning models and data pipelines. Provides visibility into data quality &amp; model performance over time. Supports privacy-preserving data collection, ensuring safety &amp; robustness. AI system Logging &amp; Monitor AI system Logging &amp; Monitor (RECICLAI) Hive The Apache Hive \u2122 is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL. ETL Airbyte The leading data integration platform for ETL / ELT data pipelines from APIs, databases &amp; files to data warehouses, data lakes &amp; data lakehouses. Both self-hosted and Cloud-hosted. Feature Engineering tsfresh tsfresh is a python package. It automatically calculates a large number of time series characteristics, the so called features. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks. Stream Processing Kafka Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. Flink Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Visualization D3 Bring data to life with SVG, Canvas and HTML. Plotly-Dash Data Apps &amp; Dashboards for Python. No JavaScript Required. Grafana The open and composable observability and data visualization platform. Visualize metrics, logs, and traces from multiple sources like Prometheus, Loki, Elasticsearch, InfluxDB, Postgres and many more. Prometheus The Prometheus monitoring system and time series database. Streamlit A faster way to build and share data apps. Kibana Run data analytics at speed and scale for observability, security, and search with Kibana. Powerful analysis on any data from any source, from threat intelligence to search analytics, logs to application monitoring, and much more. Gradio Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere! Pipeline Management TPOT TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Labeling &amp; Annotation Label Studio Label Studio is a multi-type data labeling and annotation tool with standardized output format. CVAT Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale. Supervisely Develop AI faster and better with on-premise, enterprise-grade end-to-end solution for every task: from labeling to building production models."},{"location":"toolbox/ai_toolkit/#validation","title":"Validation","text":"Validation Evidently AI Evidently helps analyze and monitor the quality of machine learning models in production. It generates detailed reports on data drift and model performance, using visualizations to identify significant changes in input data or model performance. Whylogs Whylogs is a lightweight and scalable library for logging and monitoring ML data in production. It provides statistical profiles of input and output data, facilitating the detection of data drift and anomalies in real-time or batch data. Promehteus &amp; Grafana Although not specific to ML, they can be adapted to monitor specific ML model metrics, including production accuracy. By defining custom metrics that reflect model performance, they can be used to capture and visualize data drift or model drift, though this requires manual configuration and clear metric definitions. Alibi Detect Specialized in anomaly and data drift detection, Alibi Detect offers a series of techniques and algorithms designed specifically to identify changes in input data and model behavior, which may indicate the need for retraining. MLPerf (and MLCommons) MLPerf is a suite of benchmarks that evaluates the performance of hardware, software, and machine learning models. It provides standardized metrics that allow comparing different implementations and configurations of ML, helping to identify best practices and optimizations in the field of machine learning."},{"location":"toolbox/mims_toolkit/","title":"MIMs Tools","text":"<p>List of tools and projects that can help services comply with OASC Minimal Interoperability Mechanisms.</p>"},{"location":"toolbox/mims_toolkit/#mim1-context-information","title":"MIM1 - Context Information","text":"Tool Features Orion Context Broker Orion is a widely adopted and mature context broker implementation developed by the FIWARE community. Scorpio Broker Scorpio Broker is another context broker implementation that conforms to the NGSI-LD standard. Djane.io Djane.io is an open-source context broker implementation that also supports the NGSI-LD standard. It aims to provide a flexible and extensible platform for managing context information Stellio Broker Stellio is a context broker implementation specifically designed for high-performance and large-scale deployments. Lepus An NGSI-LD wrapper for NGSI-v2 Context Brokers, which facilitates the transition or interoperability between both versions of the NGSI specification. QuantumLeap A FIWARE Generic Enabler to support the use of NGSIv2 (and NGSI-LD experimentally) data in time-series databases tutorials NGSI-LD Collection of tutorials for the FIWARE ecosystem designed for NGSI-LD developers. Orange-OpenSource/python-ngsild-client A Python library dedicated to NGSI-LD, which serves both as an NGSI-LD API client and a toolbox for efficiently creating and manipulating NGSI-LD entities. OGC SensorThings API The OGC SensorThings API is a geospatially enabled framework that connects Internet of Things (IoT) devices, data, and applications via the web. This API facilitates interoperability and seamless integration within IoT ecosystems. ngsild-client Python ngsi-ld client for UPV-CitCom.ai projects. A guide for connecting NGSI-V2 Broker to NGSI-LD via FIWARE IoT-Agent Local environment for testing subscriptions between FIWARE Orion brokers NGSI-V2 to NGSI-LD via FIWARE IoT-Agent."},{"location":"toolbox/mims_toolkit/#mim2-data-models","title":"MIM2 - Data Models","text":"Tool Features Smart Data Models An Umbrella Repository for collecting Data Models based on real world use-cases. pySmartDataModels The pysmartdatamodels package provides a comprehensive collection of open-licensed, free data models suitable for digital twin implementations, data spaces, and smart application development. This Python package includes functions for utilizing these data models in development, ensuring compatibility and simplification in digital solutions creation\u200b. SAREF (Smart Applications REFerence) ontology It is a consensus-based model designed to streamline the integration of smart applications by providing a set of modular building blocks. SAREF introduces core concepts, relationships, and axioms for the smart applications domain, emphasizing reuse, modularity, extensibility, and maintainability. DTDL DTDL is based on JSON-LD and is programming-language independent. DTDL isn't exclusive to Azure Digital Twins. It is also used to represent device data in other IoT services such as IoT Plug and Play."},{"location":"toolbox/other_toolkit/","title":"Other tools","text":"<p>List of other useful tools.</p> Tool Features NGSI-LD Grafana datasource plugin It is a Grafana datasource plugin for integrating with FIWARE/NGSI-LD context brokers. It enables the visualization of temporal, geographical, and graph data within Grafana, enhancing the analysis and monitoring capabilities in smart and connected environments. Aq\u00fceducte It is a microservice designed to extract data from diverse sources, convert them to the NGSI-LD format, and import this data for any application utilizing the NGSI-LD protocol. ngsi-ld-client It is a Python-based client for the NGSI-LD API, created from its OpenAPI specification. It is designed to interact with NGSI-LD compliant systems, facilitating operations with the NGSI-LD API through Python. Digital Twin of Industry Fusion It encompasses the development of a Digital Twin Concept aimed at facilitating NGSI-LD based entity management, along with StreamingSQL and SHACL based descriptions of processes. It involves an architectural design that integrates various components such as Kafka, EMQX/MQTT, Scorpio (NGSI-LD Broker), and others to support the digital twin framework. ngsi-ld-deployment It provides sample docker-compose files for deploying NGSI-LD environments. This is useful for setting up and configuring NGSI-LD compliant systems using Docker, streamlining the deployment process for applications that utilize the NGSI-LD protocol. streamlit-app It showcases a web interface for preprocessing data, training, and testing a machine learning model, with data provisioned in real-time via REST API and by means of deploying the FIWARE Context Broker."}]}